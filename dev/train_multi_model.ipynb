{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_multi_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87PJ8R8psm3W",
        "outputId": "b3b2804d-bd06-49f9-d284-751b4607b3cd"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "from torch import Tensor\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.fft import rfft, rfftfreq, fft, fftfreq\n",
        "import scipy\n",
        "import time\n",
        "import copy\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load in Data\n",
        "with open('/content/drive/MyDrive/CSE 481 Capstone/processed_data.npy', 'rb') as f:\n",
        "    data = np.load(f)\n",
        "print(data.shape)"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(17920, 4, 32, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzbwRk48s3jh",
        "outputId": "2900a5d8-21e1-4565-d917-0eb661aedb0b"
      },
      "source": [
        "# Load in Labels\n",
        "labels = pd.read_excel(\"/content/drive/MyDrive/CSE 481 Capstone/metadata_xls/participant_ratings.xls\")\n",
        "print(labels.head())\n",
        "sub_labels = []\n",
        "for i in range(len(labels)):\n",
        "  sub_labels.append([labels.loc[i, 'Valence'], labels.loc[i, 'Arousal']])\n",
        "sub_labels = np.array(sub_labels)\n",
        "print(sub_labels.shape)\n",
        "print(sub_labels)\n",
        "sub_labels_2 = np.zeros((len(sub_labels)))\n",
        "for i in range(len(sub_labels)):\n",
        "  instance = sub_labels[i]\n",
        "  valence = instance[0]\n",
        "  arousal = instance[1]\n",
        "  if (valence < 5 and arousal < 5):\n",
        "    sub_labels_2[i] = 0\n",
        "  elif (valence < 5 and arousal >= 5):\n",
        "    sub_labels_2[i] = 1\n",
        "  elif (valence >= 5 and arousal < 5):\n",
        "    sub_labels_2[i] = 2\n",
        "  else:\n",
        "    sub_labels_2[i] = 3\n",
        "#lb = preprocessing.LabelBinarizer()\n",
        "#sub_labels_2 = lb.fit_transform(sub_labels_2)\n",
        "print(sub_labels_2)\n",
        "\n",
        "# convert to windowed labels\n",
        "data_labels = np.repeat(sub_labels_2, 14, axis=0)\n",
        "print(data_labels.shape)\n",
        "print(data_labels)\n",
        "\n",
        "#convert to tensor\n",
        "#ata = torch.tensor(data)\n",
        "data = torch.from_numpy(data).float()\n",
        "data_labels = torch.from_numpy(data_labels).float()\n",
        "print(data.dtype)\n",
        "print(data.shape)\n",
        "dataset = TensorDataset(Tensor(data) , Tensor(data_labels))\n",
        "\n",
        "print(data[29][1])"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Participant_id  Trial  Experiment_id  ...  Dominance  Liking  Familiarity\n",
            "0               1      1              5  ...       7.19    6.05          4.0\n",
            "1               1      2             18  ...       6.94    8.01          4.0\n",
            "2               1      3              4  ...       6.12    8.06          4.0\n",
            "3               1      4             24  ...       8.01    8.22          4.0\n",
            "4               1      5             20  ...       7.19    8.13          1.0\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "(1280, 2)\n",
            "[[6.96 3.92]\n",
            " [7.23 7.15]\n",
            " [4.94 6.01]\n",
            " ...\n",
            " [8.05 7.09]\n",
            " [4.01 7.17]\n",
            " [4.08 5.95]]\n",
            "[2. 3. 1. ... 3. 1. 1.]\n",
            "(17920,)\n",
            "[2. 2. 2. ... 1. 1. 1.]\n",
            "torch.float32\n",
            "torch.Size([17920, 4, 32, 32])\n",
            "tensor([[ 1.0000,  0.6335,  0.4222,  ..., -0.0332,  0.3070,  0.1884],\n",
            "        [ 0.6335,  1.0000,  0.8858,  ..., -0.0963,  0.4528,  0.2397],\n",
            "        [ 0.4222,  0.8858,  1.0000,  ...,  0.0902,  0.4818,  0.2817],\n",
            "        ...,\n",
            "        [-0.0332, -0.0963,  0.0902,  ...,  1.0000,  0.4132,  0.5115],\n",
            "        [ 0.3070,  0.4528,  0.4818,  ...,  0.4132,  1.0000,  0.7692],\n",
            "        [ 0.1884,  0.2397,  0.2817,  ...,  0.5115,  0.7692,  1.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ9ewnx7s8iI",
        "outputId": "6bb398ee-ae6d-4b55-82f6-71717e8706fa"
      },
      "source": [
        "pre_train_size = int(0.9 * len(dataset))\n",
        "test_size = len(dataset) - pre_train_size\n",
        "pre_train_set, test_set = torch.utils.data.random_split(dataset, [pre_train_size, test_size])\n",
        "train_size = int(0.8 * len(pre_train_set))\n",
        "val_size = len(pre_train_set) - train_size\n",
        "print(len(pre_train_set))\n",
        "print(train_size)\n",
        "print(val_size)\n",
        "train_set, val_set = torch.utils.data.random_split(pre_train_set, [train_size, val_size])\n",
        "\n",
        "batch_size = 128\n",
        "print(len(train_set), len(val_set), len(test_set))\n",
        "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "dataloaders = {\n",
        "    'train': trainloader,\n",
        "    'val': valloader,\n",
        "}"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16128\n",
            "12902\n",
            "3226\n",
            "12902 3226 1792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uZ8lgkJhnEx",
        "outputId": "95cf2946-a03c-4e5d-e58b-d113ce7ff369"
      },
      "source": [
        "# with auto encoder\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# skipped autoencoder\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(4, 32, [3, 1]),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Conv2d(32, 64, [3, 1]),\n",
        "    nn.ReLU(), # Maybe not sure\n",
        "    nn.Dropout(),\n",
        "    nn.MaxPool2d([3, 3]),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(5760, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 4)\n",
        ")\n",
        "model.to(device)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "print(model)"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Sequential(\n",
            "  (0): Conv2d(4, 32, kernel_size=[3, 1], stride=(1, 1))\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Conv2d(32, 64, kernel_size=[3, 1], stride=(1, 1))\n",
            "  (4): ReLU()\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): MaxPool2d(kernel_size=[3, 3], stride=[3, 3], padding=0, dilation=1, ceil_mode=False)\n",
            "  (7): Flatten(start_dim=1, end_dim=-1)\n",
            "  (8): Linear(in_features=5760, out_features=512, bias=True)\n",
            "  (9): ReLU()\n",
            "  (10): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (11): ReLU()\n",
            "  (12): Linear(in_features=256, out_features=4, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "318qI3rVtD-L",
        "outputId": "a580c9c5-9bd8-4d79-fd56-0f7982290cd3"
      },
      "source": [
        "since = time.time()\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = 10000.0\n",
        "all_train_loss = []\n",
        "all_val_loss = []\n",
        "num_epochs = 75\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in dataloaders[phase]:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            #print(inputs)\n",
        "            #print(labels)\n",
        "            \n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs = model(inputs)\n",
        "                #print(outputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                #print(preds)\n",
        "                #print(outputs.dtype)\n",
        "                #print(labels.dtype)\n",
        "                loss = loss_func(outputs, labels.long())\n",
        "                #print(loss.item())\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                #print(running_loss)\n",
        "                \n",
        "                running_corrects += torch.sum(preds == labels.data) / inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders[phase])\n",
        "        epoch_acc = running_corrects / len(dataloaders[phase])\n",
        "\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "            phase, epoch_loss, epoch_acc))\n",
        "        if phase == 'train':\n",
        "          all_train_loss.append(epoch_loss)\n",
        "        else:\n",
        "          all_val_loss.append(epoch_loss)\n",
        "        # deep copy the model\n",
        "        # if phase == 'val' and epoch_acc > best_acc:\n",
        "        #     best_acc = epoch_acc\n",
        "        #     best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        if phase == 'val' and epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print()\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))\n",
        "print('Best val Loss: {:4f}'.format(best_loss))\n",
        "\n",
        "# load best model weights\n",
        "model.load_state_dict(best_model_wts)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/74\n",
            "----------\n",
            "train Loss: 1.3677 Acc: 0.3521\n",
            "val Loss: 1.3808 Acc: 0.3137\n",
            "\n",
            "Epoch 1/74\n",
            "----------\n",
            "train Loss: 1.3344 Acc: 0.3667\n",
            "val Loss: 1.3465 Acc: 0.3667\n",
            "\n",
            "Epoch 2/74\n",
            "----------\n",
            "train Loss: 1.2942 Acc: 0.3884\n",
            "val Loss: 1.3178 Acc: 0.3732\n",
            "\n",
            "Epoch 3/74\n",
            "----------\n",
            "train Loss: 1.2708 Acc: 0.3979\n",
            "val Loss: 1.3050 Acc: 0.3880\n",
            "\n",
            "Epoch 4/74\n",
            "----------\n",
            "train Loss: 1.2573 Acc: 0.4057\n",
            "val Loss: 1.2920 Acc: 0.4081\n",
            "\n",
            "Epoch 5/74\n",
            "----------\n",
            "train Loss: 1.2428 Acc: 0.4152\n",
            "val Loss: 1.2791 Acc: 0.4052\n",
            "\n",
            "Epoch 6/74\n",
            "----------\n",
            "train Loss: 1.2340 Acc: 0.4212\n",
            "val Loss: 1.2629 Acc: 0.4088\n",
            "\n",
            "Epoch 7/74\n",
            "----------\n",
            "train Loss: 1.2212 Acc: 0.4282\n",
            "val Loss: 1.2591 Acc: 0.4183\n",
            "\n",
            "Epoch 8/74\n",
            "----------\n",
            "train Loss: 1.2122 Acc: 0.4306\n",
            "val Loss: 1.2474 Acc: 0.4327\n",
            "\n",
            "Epoch 9/74\n",
            "----------\n",
            "train Loss: 1.1973 Acc: 0.4408\n",
            "val Loss: 1.2579 Acc: 0.4384\n",
            "\n",
            "Epoch 10/74\n",
            "----------\n",
            "train Loss: 1.1892 Acc: 0.4475\n",
            "val Loss: 1.2363 Acc: 0.4235\n",
            "\n",
            "Epoch 11/74\n",
            "----------\n",
            "train Loss: 1.1807 Acc: 0.4493\n",
            "val Loss: 1.2393 Acc: 0.4241\n",
            "\n",
            "Epoch 12/74\n",
            "----------\n",
            "train Loss: 1.1626 Acc: 0.4612\n",
            "val Loss: 1.2089 Acc: 0.4457\n",
            "\n",
            "Epoch 13/74\n",
            "----------\n",
            "train Loss: 1.1461 Acc: 0.4719\n",
            "val Loss: 1.2106 Acc: 0.4393\n",
            "\n",
            "Epoch 14/74\n",
            "----------\n",
            "train Loss: 1.1362 Acc: 0.4748\n",
            "val Loss: 1.2004 Acc: 0.4541\n",
            "\n",
            "Epoch 15/74\n",
            "----------\n",
            "train Loss: 1.1110 Acc: 0.4972\n",
            "val Loss: 1.2002 Acc: 0.4529\n",
            "\n",
            "Epoch 16/74\n",
            "----------\n",
            "train Loss: 1.0930 Acc: 0.5025\n",
            "val Loss: 1.1938 Acc: 0.4592\n",
            "\n",
            "Epoch 17/74\n",
            "----------\n",
            "train Loss: 1.0763 Acc: 0.5151\n",
            "val Loss: 1.1838 Acc: 0.4618\n",
            "\n",
            "Epoch 18/74\n",
            "----------\n",
            "train Loss: 1.0608 Acc: 0.5238\n",
            "val Loss: 1.1770 Acc: 0.4664\n",
            "\n",
            "Epoch 19/74\n",
            "----------\n",
            "train Loss: 1.0375 Acc: 0.5404\n",
            "val Loss: 1.1789 Acc: 0.4611\n",
            "\n",
            "Epoch 20/74\n",
            "----------\n",
            "train Loss: 1.0146 Acc: 0.5491\n",
            "val Loss: 1.1597 Acc: 0.4813\n",
            "\n",
            "Epoch 21/74\n",
            "----------\n",
            "train Loss: 1.0009 Acc: 0.5596\n",
            "val Loss: 1.1640 Acc: 0.4867\n",
            "\n",
            "Epoch 22/74\n",
            "----------\n",
            "train Loss: 0.9771 Acc: 0.5720\n",
            "val Loss: 1.1502 Acc: 0.4864\n",
            "\n",
            "Epoch 23/74\n",
            "----------\n",
            "train Loss: 0.9598 Acc: 0.5820\n",
            "val Loss: 1.1490 Acc: 0.4865\n",
            "\n",
            "Epoch 24/74\n",
            "----------\n",
            "train Loss: 0.9301 Acc: 0.5966\n",
            "val Loss: 1.1537 Acc: 0.4955\n",
            "\n",
            "Epoch 25/74\n",
            "----------\n",
            "train Loss: 0.9221 Acc: 0.6013\n",
            "val Loss: 1.1485 Acc: 0.4823\n",
            "\n",
            "Epoch 26/74\n",
            "----------\n",
            "train Loss: 0.8914 Acc: 0.6183\n",
            "val Loss: 1.1431 Acc: 0.4819\n",
            "\n",
            "Epoch 27/74\n",
            "----------\n",
            "train Loss: 0.8795 Acc: 0.6201\n",
            "val Loss: 1.1434 Acc: 0.4790\n",
            "\n",
            "Epoch 28/74\n",
            "----------\n",
            "train Loss: 0.8439 Acc: 0.6425\n",
            "val Loss: 1.1452 Acc: 0.4882\n",
            "\n",
            "Epoch 29/74\n",
            "----------\n",
            "train Loss: 0.8277 Acc: 0.6471\n",
            "val Loss: 1.1502 Acc: 0.4836\n",
            "\n",
            "Epoch 30/74\n",
            "----------\n",
            "train Loss: 0.8007 Acc: 0.6619\n",
            "val Loss: 1.1432 Acc: 0.5038\n",
            "\n",
            "Epoch 31/74\n",
            "----------\n",
            "train Loss: 0.7879 Acc: 0.6687\n",
            "val Loss: 1.1693 Acc: 0.5006\n",
            "\n",
            "Epoch 32/74\n",
            "----------\n",
            "train Loss: 0.7548 Acc: 0.6805\n",
            "val Loss: 1.1604 Acc: 0.4905\n",
            "\n",
            "Epoch 33/74\n",
            "----------\n",
            "train Loss: 0.7369 Acc: 0.6927\n",
            "val Loss: 1.1482 Acc: 0.4985\n",
            "\n",
            "Epoch 34/74\n",
            "----------\n",
            "train Loss: 0.7174 Acc: 0.7002\n",
            "val Loss: 1.1419 Acc: 0.4979\n",
            "\n",
            "Epoch 35/74\n",
            "----------\n",
            "train Loss: 0.7046 Acc: 0.7079\n",
            "val Loss: 1.1519 Acc: 0.4916\n",
            "\n",
            "Epoch 36/74\n",
            "----------\n",
            "train Loss: 0.6838 Acc: 0.7177\n",
            "val Loss: 1.1801 Acc: 0.4832\n",
            "\n",
            "Epoch 37/74\n",
            "----------\n",
            "train Loss: 0.6701 Acc: 0.7249\n",
            "val Loss: 1.1843 Acc: 0.4818\n",
            "\n",
            "Epoch 38/74\n",
            "----------\n",
            "train Loss: 0.6496 Acc: 0.7331\n",
            "val Loss: 1.1735 Acc: 0.4915\n",
            "\n",
            "Epoch 39/74\n",
            "----------\n",
            "train Loss: 0.6370 Acc: 0.7345\n",
            "val Loss: 1.1386 Acc: 0.4994\n",
            "\n",
            "Epoch 40/74\n",
            "----------\n",
            "train Loss: 0.6187 Acc: 0.7493\n",
            "val Loss: 1.1851 Acc: 0.5026\n",
            "\n",
            "Epoch 41/74\n",
            "----------\n",
            "train Loss: 0.6149 Acc: 0.7477\n",
            "val Loss: 1.1735 Acc: 0.5009\n",
            "\n",
            "Epoch 42/74\n",
            "----------\n",
            "train Loss: 0.5951 Acc: 0.7582\n",
            "val Loss: 1.1880 Acc: 0.5003\n",
            "\n",
            "Epoch 43/74\n",
            "----------\n",
            "train Loss: 0.5885 Acc: 0.7609\n",
            "val Loss: 1.1919 Acc: 0.4973\n",
            "\n",
            "Epoch 44/74\n",
            "----------\n",
            "train Loss: 0.5579 Acc: 0.7733\n",
            "val Loss: 1.1926 Acc: 0.4900\n",
            "\n",
            "Epoch 45/74\n",
            "----------\n",
            "train Loss: 0.5516 Acc: 0.7767\n",
            "val Loss: 1.2046 Acc: 0.4976\n",
            "\n",
            "Epoch 46/74\n",
            "----------\n",
            "train Loss: 0.5443 Acc: 0.7780\n",
            "val Loss: 1.2150 Acc: 0.4883\n",
            "\n",
            "Epoch 47/74\n",
            "----------\n",
            "train Loss: 0.5278 Acc: 0.7851\n",
            "val Loss: 1.1943 Acc: 0.4982\n",
            "\n",
            "Epoch 48/74\n",
            "----------\n",
            "train Loss: 0.5213 Acc: 0.7906\n",
            "val Loss: 1.2161 Acc: 0.5054\n",
            "\n",
            "Epoch 49/74\n",
            "----------\n",
            "train Loss: 0.5050 Acc: 0.7956\n",
            "val Loss: 1.2304 Acc: 0.5063\n",
            "\n",
            "Epoch 50/74\n",
            "----------\n",
            "train Loss: 0.4949 Acc: 0.8013\n",
            "val Loss: 1.2233 Acc: 0.5006\n",
            "\n",
            "Epoch 51/74\n",
            "----------\n",
            "train Loss: 0.4996 Acc: 0.8006\n",
            "val Loss: 1.2370 Acc: 0.4838\n",
            "\n",
            "Epoch 52/74\n",
            "----------\n",
            "train Loss: 0.4760 Acc: 0.8088\n",
            "val Loss: 1.2456 Acc: 0.5041\n",
            "\n",
            "Epoch 53/74\n",
            "----------\n",
            "train Loss: 0.4685 Acc: 0.8124\n",
            "val Loss: 1.2357 Acc: 0.5021\n",
            "\n",
            "Epoch 54/74\n",
            "----------\n",
            "train Loss: 0.4591 Acc: 0.8183\n",
            "val Loss: 1.2508 Acc: 0.4958\n",
            "\n",
            "Epoch 55/74\n",
            "----------\n",
            "train Loss: 0.4531 Acc: 0.8174\n",
            "val Loss: 1.2553 Acc: 0.5003\n",
            "\n",
            "Epoch 56/74\n",
            "----------\n",
            "train Loss: 0.4434 Acc: 0.8251\n",
            "val Loss: 1.2393 Acc: 0.5239\n",
            "\n",
            "Epoch 57/74\n",
            "----------\n",
            "train Loss: 0.4239 Acc: 0.8308\n",
            "val Loss: 1.2740 Acc: 0.5006\n",
            "\n",
            "Epoch 58/74\n",
            "----------\n",
            "train Loss: 0.4211 Acc: 0.8315\n",
            "val Loss: 1.2815 Acc: 0.5012\n",
            "\n",
            "Epoch 59/74\n",
            "----------\n",
            "train Loss: 0.4267 Acc: 0.8299\n",
            "val Loss: 1.3023 Acc: 0.4968\n",
            "\n",
            "Epoch 60/74\n",
            "----------\n",
            "train Loss: 0.4294 Acc: 0.8325\n",
            "val Loss: 1.2721 Acc: 0.4957\n",
            "\n",
            "Epoch 61/74\n",
            "----------\n",
            "train Loss: 0.4115 Acc: 0.8359\n",
            "val Loss: 1.2818 Acc: 0.5000\n",
            "\n",
            "Epoch 62/74\n",
            "----------\n",
            "train Loss: 0.3910 Acc: 0.8439\n",
            "val Loss: 1.2959 Acc: 0.4980\n",
            "\n",
            "Epoch 63/74\n",
            "----------\n",
            "train Loss: 0.3915 Acc: 0.8452\n",
            "val Loss: 1.2935 Acc: 0.5107\n",
            "\n",
            "Epoch 64/74\n",
            "----------\n",
            "train Loss: 0.3909 Acc: 0.8465\n",
            "val Loss: 1.3187 Acc: 0.5078\n",
            "\n",
            "Epoch 65/74\n",
            "----------\n",
            "train Loss: 0.3846 Acc: 0.8480\n",
            "val Loss: 1.3081 Acc: 0.5012\n",
            "\n",
            "Epoch 66/74\n",
            "----------\n",
            "train Loss: 0.3759 Acc: 0.8524\n",
            "val Loss: 1.3026 Acc: 0.5153\n",
            "\n",
            "Epoch 67/74\n",
            "----------\n",
            "train Loss: 0.3622 Acc: 0.8586\n",
            "val Loss: 1.3416 Acc: 0.5097\n",
            "\n",
            "Epoch 68/74\n",
            "----------\n",
            "train Loss: 0.3732 Acc: 0.8521\n",
            "val Loss: 1.3308 Acc: 0.5151\n",
            "\n",
            "Epoch 69/74\n",
            "----------\n",
            "train Loss: 0.3510 Acc: 0.8633\n",
            "val Loss: 1.3266 Acc: 0.5030\n",
            "\n",
            "Epoch 70/74\n",
            "----------\n",
            "train Loss: 0.3569 Acc: 0.8605\n",
            "val Loss: 1.3335 Acc: 0.5087\n",
            "\n",
            "Epoch 71/74\n",
            "----------\n",
            "train Loss: 0.3491 Acc: 0.8656\n",
            "val Loss: 1.3425 Acc: 0.4985\n",
            "\n",
            "Epoch 72/74\n",
            "----------\n",
            "train Loss: 0.3403 Acc: 0.8657\n",
            "val Loss: 1.3603 Acc: 0.5027\n",
            "\n",
            "Epoch 73/74\n",
            "----------\n",
            "train Loss: 0.3326 Acc: 0.8703\n",
            "val Loss: 1.3690 Acc: 0.5102\n",
            "\n",
            "Epoch 74/74\n",
            "----------\n",
            "train Loss: 0.3272 Acc: 0.8739\n",
            "val Loss: 1.4088 Acc: 0.5101\n",
            "\n",
            "Training complete in 2m 13s\n",
            "Best val Loss: 1.138594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "SFnswSmvgnwA",
        "outputId": "a9448a96-00bb-47b8-e591-f0b319b327c7"
      },
      "source": [
        "plt.plot(all_train_loss)\n",
        "plt.plot(all_val_loss)\n",
        "plt.legend(['train', 'val'])"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa2fcea2250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPyHYl6NxQ2w",
        "outputId": "0cf6fc55-69cd-4755-f6fa-2b8b7fea58fc"
      },
      "source": [
        "running_loss = 0.0\n",
        "running_corrects = 0\n",
        "model.eval()\n",
        "\n",
        "# Iterate over data.\n",
        "for inputs, labels in testloader:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(inputs)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    loss = loss_func(outputs, labels.long())\n",
        "\n",
        "    # statistics\n",
        "    running_loss += loss.item()\n",
        "    running_corrects += torch.sum(preds == labels.data) / inputs.size(0)\n",
        "epoch_loss = running_loss / len(testloader)\n",
        "print('Test Loss: {:.4f}'.format(epoch_loss))\n",
        "print('Test Acc: {:.4f}'.format(running_corrects / len(testloader)))\n",
        "print()"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 1.1105\n",
            "Test Acc: 0.5128\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGe-2lnyhDGq"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/CSE 481 Capstone/model_multi.pth')"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yluuG3bBmE40",
        "outputId": "2f3b7311-57f8-46ea-8b29-26cc123e0707"
      },
      "source": [
        "model.eval()\n",
        "datatestiter = iter(testloader)\n",
        "input_test, labels_test = datatestiter.next()\n",
        "input_test = input_test.to(device)\n",
        "labels_test = labels_test.to(device)\n",
        "output_test = model(input_test)\n",
        "_, preds = torch.max(output_test, 1)\n",
        "print(preds.shape)\n",
        "print(labels_test.shape)\n",
        "print('preds',preds[0:8])\n",
        "print('labels', labels_test[0:8])\n",
        "print(torch.sum(preds == labels_test) / len(labels_test))"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "preds tensor([3, 3, 3, 3, 0, 2, 3, 1], device='cuda:0')\n",
            "labels tensor([3., 3., 3., 2., 2., 2., 3., 1.], device='cuda:0')\n",
            "tensor(0.5781, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}