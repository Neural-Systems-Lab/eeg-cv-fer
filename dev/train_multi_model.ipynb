{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_multi_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87PJ8R8psm3W",
        "outputId": "3a70d519-5ad5-4c81-f595-0dba12b5ad8c"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "from torch import Tensor\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.fft import rfft, rfftfreq, fft, fftfreq\n",
        "import scipy\n",
        "import time\n",
        "import copy\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load in Data\n",
        "with open('/content/drive/MyDrive/CSE 481 Capstone/processed_data.npy', 'rb') as f:\n",
        "    data = np.load(f)\n",
        "print(data.shape)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(17920, 4, 32, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzbwRk48s3jh",
        "outputId": "372b26de-982b-4e34-f947-89ce0a764657"
      },
      "source": [
        "# Load in Labels\n",
        "labels = pd.read_excel(\"/content/drive/MyDrive/CSE 481 Capstone/metadata_xls/participant_ratings.xls\")\n",
        "print(labels.head())\n",
        "sub_labels = []\n",
        "for i in range(len(labels)):\n",
        "  sub_labels.append([labels.loc[i, 'Valence'], labels.loc[i, 'Arousal']])\n",
        "sub_labels = np.array(sub_labels)\n",
        "print(sub_labels.shape)\n",
        "print(sub_labels)\n",
        "sub_labels_2 = np.zeros((len(sub_labels)))\n",
        "for i in range(len(sub_labels)):\n",
        "  instance = sub_labels[i]\n",
        "  valence = instance[0]\n",
        "  arousal = instance[1]\n",
        "  if (valence < 5 and arousal < 5):\n",
        "    sub_labels_2[i] = 0\n",
        "  elif (valence < 5 and arousal >= 5):\n",
        "    sub_labels_2[i] = 1\n",
        "  elif (valence >= 5 and arousal < 5):\n",
        "    sub_labels_2[i] = 2\n",
        "  else:\n",
        "    sub_labels_2[i] = 3\n",
        "#lb = preprocessing.LabelBinarizer()\n",
        "#sub_labels_2 = lb.fit_transform(sub_labels_2)\n",
        "print(sub_labels_2)\n",
        "\n",
        "# convert to windowed labels\n",
        "data_labels = np.repeat(sub_labels_2, 14, axis=0)\n",
        "print(data_labels.shape)\n",
        "print(data_labels)\n",
        "\n",
        "#convert to tensor\n",
        "#ata = torch.tensor(data)\n",
        "data = torch.from_numpy(data).float()\n",
        "data_labels = torch.from_numpy(data_labels).float()\n",
        "print(data.dtype)\n",
        "print(data.shape)\n",
        "dataset = TensorDataset(Tensor(data) , Tensor(data_labels))\n",
        "\n",
        "print(data[29][1])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Participant_id  Trial  Experiment_id  ...  Dominance  Liking  Familiarity\n",
            "0               1      1              5  ...       7.19    6.05          4.0\n",
            "1               1      2             18  ...       6.94    8.01          4.0\n",
            "2               1      3              4  ...       6.12    8.06          4.0\n",
            "3               1      4             24  ...       8.01    8.22          4.0\n",
            "4               1      5             20  ...       7.19    8.13          1.0\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "(1280, 2)\n",
            "[[6.96 3.92]\n",
            " [7.23 7.15]\n",
            " [4.94 6.01]\n",
            " ...\n",
            " [8.05 7.09]\n",
            " [4.01 7.17]\n",
            " [4.08 5.95]]\n",
            "[2. 3. 1. ... 3. 1. 1.]\n",
            "(17920,)\n",
            "[2. 2. 2. ... 1. 1. 1.]\n",
            "torch.float32\n",
            "torch.Size([17920, 4, 32, 32])\n",
            "tensor([[ 1.0000,  0.6335,  0.4222,  ..., -0.0332,  0.3070,  0.1884],\n",
            "        [ 0.6335,  1.0000,  0.8858,  ..., -0.0963,  0.4528,  0.2397],\n",
            "        [ 0.4222,  0.8858,  1.0000,  ...,  0.0902,  0.4818,  0.2817],\n",
            "        ...,\n",
            "        [-0.0332, -0.0963,  0.0902,  ...,  1.0000,  0.4132,  0.5115],\n",
            "        [ 0.3070,  0.4528,  0.4818,  ...,  0.4132,  1.0000,  0.7692],\n",
            "        [ 0.1884,  0.2397,  0.2817,  ...,  0.5115,  0.7692,  1.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ9ewnx7s8iI",
        "outputId": "f098f3bf-4ea2-4e35-f0cb-0084b3d6ef8d"
      },
      "source": [
        "pre_train_size = int(0.9 * len(dataset))\n",
        "test_size = len(dataset) - pre_train_size\n",
        "pre_train_set, test_set = torch.utils.data.random_split(dataset, [pre_train_size, test_size])\n",
        "train_size = int(0.8 * len(pre_train_set))\n",
        "val_size = len(pre_train_set) - train_size\n",
        "print(len(pre_train_set))\n",
        "print(train_size)\n",
        "print(val_size)\n",
        "train_set, val_set = torch.utils.data.random_split(pre_train_set, [train_size, val_size])\n",
        "\n",
        "batch_size = 128\n",
        "print(len(train_set), len(val_set), len(test_set))\n",
        "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "dataloaders = {\n",
        "    'train': trainloader,\n",
        "    'val': valloader,\n",
        "}"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16128\n",
            "12902\n",
            "3226\n",
            "12902 3226 1792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uZ8lgkJhnEx",
        "outputId": "8ebfdcfb-4298-42e8-eb67-d0b2fc3c9b44"
      },
      "source": [
        "# with auto encoder\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# skipped autoencoder\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(4, 32, [3, 1]),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Conv2d(32, 64, [3, 1]),\n",
        "    nn.ReLU(), # Maybe not sure\n",
        "    nn.Dropout(),\n",
        "    nn.MaxPool2d([3, 3]),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(5760, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 4)\n",
        ")\n",
        "model.to(device)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "print(model)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Sequential(\n",
            "  (0): Conv2d(4, 32, kernel_size=[3, 1], stride=(1, 1))\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Conv2d(32, 64, kernel_size=[3, 1], stride=(1, 1))\n",
            "  (4): ReLU()\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): MaxPool2d(kernel_size=[3, 3], stride=[3, 3], padding=0, dilation=1, ceil_mode=False)\n",
            "  (7): Flatten(start_dim=1, end_dim=-1)\n",
            "  (8): Linear(in_features=5760, out_features=512, bias=True)\n",
            "  (9): ReLU()\n",
            "  (10): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (11): ReLU()\n",
            "  (12): Linear(in_features=256, out_features=4, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "318qI3rVtD-L",
        "outputId": "8fdb6a3c-4580-4da8-8aea-407ea5488258"
      },
      "source": [
        "since = time.time()\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = 10000.0\n",
        "all_train_loss = []\n",
        "all_val_loss = []\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in dataloaders[phase]:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            #print(inputs)\n",
        "            #print(labels)\n",
        "            \n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs = model(inputs)\n",
        "                #print(outputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                #print(preds)\n",
        "                #print(outputs.dtype)\n",
        "                #print(labels.dtype)\n",
        "                loss = loss_func(outputs, labels.long())\n",
        "                #print(loss.item())\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                #print(running_loss)\n",
        "                \n",
        "                running_corrects += torch.sum(preds == labels.data) / inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders[phase])\n",
        "        epoch_acc = running_corrects / len(dataloaders[phase])\n",
        "\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "            phase, epoch_loss, epoch_acc))\n",
        "        if phase == 'train':\n",
        "          all_train_loss.append(epoch_loss)\n",
        "        else:\n",
        "          all_val_loss.append(epoch_loss)\n",
        "        # deep copy the model\n",
        "        # if phase == 'val' and epoch_acc > best_acc:\n",
        "        #     best_acc = epoch_acc\n",
        "        #     best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        if phase == 'val' and epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print()\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))\n",
        "print('Best val Loss: {:4f}'.format(best_loss))\n",
        "\n",
        "# load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/CSE 481 Capstone/model_multi.pth')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 1.3679 Acc: 0.3516\n",
            "val Loss: 1.3678 Acc: 0.3717\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 1.3379 Acc: 0.3701\n",
            "val Loss: 1.3420 Acc: 0.4086\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 1.2967 Acc: 0.3927\n",
            "val Loss: 1.3328 Acc: 0.3974\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 1.2792 Acc: 0.3935\n",
            "val Loss: 1.2998 Acc: 0.4105\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 1.2618 Acc: 0.4000\n",
            "val Loss: 1.2813 Acc: 0.4126\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 1.2544 Acc: 0.4076\n",
            "val Loss: 1.2654 Acc: 0.4231\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 1.2413 Acc: 0.4129\n",
            "val Loss: 1.2579 Acc: 0.4444\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 1.2270 Acc: 0.4218\n",
            "val Loss: 1.2469 Acc: 0.4268\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 1.2164 Acc: 0.4312\n",
            "val Loss: 1.2423 Acc: 0.4450\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 1.2031 Acc: 0.4364\n",
            "val Loss: 1.2271 Acc: 0.4591\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 1.1974 Acc: 0.4469\n",
            "val Loss: 1.2229 Acc: 0.4567\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 1.1786 Acc: 0.4563\n",
            "val Loss: 1.2205 Acc: 0.4511\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 1.1670 Acc: 0.4606\n",
            "val Loss: 1.2132 Acc: 0.4505\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 1.1524 Acc: 0.4706\n",
            "val Loss: 1.2090 Acc: 0.4672\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 1.1365 Acc: 0.4854\n",
            "val Loss: 1.1873 Acc: 0.4727\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 1.1202 Acc: 0.4961\n",
            "val Loss: 1.1918 Acc: 0.4602\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 1.1039 Acc: 0.5051\n",
            "val Loss: 1.1914 Acc: 0.4595\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 1.0829 Acc: 0.5145\n",
            "val Loss: 1.1815 Acc: 0.4718\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 1.0611 Acc: 0.5230\n",
            "val Loss: 1.1688 Acc: 0.4685\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 1.0388 Acc: 0.5367\n",
            "val Loss: 1.1631 Acc: 0.4730\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 1.0227 Acc: 0.5487\n",
            "val Loss: 1.1578 Acc: 0.4805\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 0.9977 Acc: 0.5599\n",
            "val Loss: 1.1521 Acc: 0.4654\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 0.9819 Acc: 0.5723\n",
            "val Loss: 1.1465 Acc: 0.4706\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 0.9551 Acc: 0.5869\n",
            "val Loss: 1.1450 Acc: 0.4813\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 0.9355 Acc: 0.5918\n",
            "val Loss: 1.1429 Acc: 0.4781\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 0.9154 Acc: 0.6056\n",
            "val Loss: 1.1293 Acc: 0.4865\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 0.8911 Acc: 0.6169\n",
            "val Loss: 1.1544 Acc: 0.4712\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 0.8705 Acc: 0.6237\n",
            "val Loss: 1.1226 Acc: 0.4891\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 0.8550 Acc: 0.6313\n",
            "val Loss: 1.1348 Acc: 0.4892\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 0.8229 Acc: 0.6476\n",
            "val Loss: 1.1501 Acc: 0.4775\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 0.8073 Acc: 0.6558\n",
            "val Loss: 1.1304 Acc: 0.4827\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 0.7935 Acc: 0.6608\n",
            "val Loss: 1.1276 Acc: 0.5010\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 0.7691 Acc: 0.6750\n",
            "val Loss: 1.1284 Acc: 0.4934\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 0.7408 Acc: 0.6893\n",
            "val Loss: 1.1435 Acc: 0.4869\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 0.7295 Acc: 0.6899\n",
            "val Loss: 1.1282 Acc: 0.4949\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 0.7159 Acc: 0.6968\n",
            "val Loss: 1.1386 Acc: 0.4823\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 0.7085 Acc: 0.7045\n",
            "val Loss: 1.1473 Acc: 0.4889\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 0.6724 Acc: 0.7196\n",
            "val Loss: 1.1690 Acc: 0.4848\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 0.6616 Acc: 0.7238\n",
            "val Loss: 1.1476 Acc: 0.4882\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 0.6441 Acc: 0.7378\n",
            "val Loss: 1.1570 Acc: 0.4847\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 0.6288 Acc: 0.7385\n",
            "val Loss: 1.1471 Acc: 0.4943\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 0.6088 Acc: 0.7494\n",
            "val Loss: 1.1622 Acc: 0.4877\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 0.5948 Acc: 0.7562\n",
            "val Loss: 1.1925 Acc: 0.4832\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 0.5908 Acc: 0.7576\n",
            "val Loss: 1.1733 Acc: 0.4880\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 0.5743 Acc: 0.7671\n",
            "val Loss: 1.1885 Acc: 0.4913\n",
            "\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 0.5633 Acc: 0.7700\n",
            "val Loss: 1.1688 Acc: 0.4957\n",
            "\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 0.5479 Acc: 0.7762\n",
            "val Loss: 1.1660 Acc: 0.5053\n",
            "\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 0.5391 Acc: 0.7790\n",
            "val Loss: 1.1882 Acc: 0.4924\n",
            "\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 0.5257 Acc: 0.7847\n",
            "val Loss: 1.2008 Acc: 0.4876\n",
            "\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 0.5108 Acc: 0.7910\n",
            "val Loss: 1.2133 Acc: 0.4864\n",
            "\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 0.4914 Acc: 0.8004\n",
            "val Loss: 1.2064 Acc: 0.5024\n",
            "\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 0.4997 Acc: 0.7965\n",
            "val Loss: 1.2163 Acc: 0.4946\n",
            "\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 0.4828 Acc: 0.8062\n",
            "val Loss: 1.2177 Acc: 0.5048\n",
            "\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 0.4771 Acc: 0.8110\n",
            "val Loss: 1.2189 Acc: 0.4871\n",
            "\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 0.4686 Acc: 0.8101\n",
            "val Loss: 1.2266 Acc: 0.4889\n",
            "\n",
            "Epoch 55/99\n",
            "----------\n",
            "train Loss: 0.4477 Acc: 0.8190\n",
            "val Loss: 1.2335 Acc: 0.5056\n",
            "\n",
            "Epoch 56/99\n",
            "----------\n",
            "train Loss: 0.4520 Acc: 0.8200\n",
            "val Loss: 1.2520 Acc: 0.4925\n",
            "\n",
            "Epoch 57/99\n",
            "----------\n",
            "train Loss: 0.4519 Acc: 0.8193\n",
            "val Loss: 1.2744 Acc: 0.4844\n",
            "\n",
            "Epoch 58/99\n",
            "----------\n",
            "train Loss: 0.4303 Acc: 0.8269\n",
            "val Loss: 1.2768 Acc: 0.5092\n",
            "\n",
            "Epoch 59/99\n",
            "----------\n",
            "train Loss: 0.4202 Acc: 0.8310\n",
            "val Loss: 1.2720 Acc: 0.4901\n",
            "\n",
            "Epoch 60/99\n",
            "----------\n",
            "train Loss: 0.4036 Acc: 0.8412\n",
            "val Loss: 1.3227 Acc: 0.4985\n",
            "\n",
            "Epoch 61/99\n",
            "----------\n",
            "train Loss: 0.4044 Acc: 0.8412\n",
            "val Loss: 1.2892 Acc: 0.4925\n",
            "\n",
            "Epoch 62/99\n",
            "----------\n",
            "train Loss: 0.3944 Acc: 0.8424\n",
            "val Loss: 1.3118 Acc: 0.4832\n",
            "\n",
            "Epoch 63/99\n",
            "----------\n",
            "train Loss: 0.3902 Acc: 0.8453\n",
            "val Loss: 1.3179 Acc: 0.4916\n",
            "\n",
            "Epoch 64/99\n",
            "----------\n",
            "train Loss: 0.3823 Acc: 0.8478\n",
            "val Loss: 1.3197 Acc: 0.5014\n",
            "\n",
            "Epoch 65/99\n",
            "----------\n",
            "train Loss: 0.3753 Acc: 0.8501\n",
            "val Loss: 1.3206 Acc: 0.5050\n",
            "\n",
            "Epoch 66/99\n",
            "----------\n",
            "train Loss: 0.3681 Acc: 0.8520\n",
            "val Loss: 1.3447 Acc: 0.5057\n",
            "\n",
            "Epoch 67/99\n",
            "----------\n",
            "train Loss: 0.3710 Acc: 0.8509\n",
            "val Loss: 1.3186 Acc: 0.4985\n",
            "\n",
            "Epoch 68/99\n",
            "----------\n",
            "train Loss: 0.3566 Acc: 0.8604\n",
            "val Loss: 1.3432 Acc: 0.5060\n",
            "\n",
            "Epoch 69/99\n",
            "----------\n",
            "train Loss: 0.3656 Acc: 0.8569\n",
            "val Loss: 1.3361 Acc: 0.4944\n",
            "\n",
            "Epoch 70/99\n",
            "----------\n",
            "train Loss: 0.3468 Acc: 0.8623\n",
            "val Loss: 1.3477 Acc: 0.4970\n",
            "\n",
            "Epoch 71/99\n",
            "----------\n",
            "train Loss: 0.3306 Acc: 0.8726\n",
            "val Loss: 1.3873 Acc: 0.4886\n",
            "\n",
            "Epoch 72/99\n",
            "----------\n",
            "train Loss: 0.3441 Acc: 0.8618\n",
            "val Loss: 1.3424 Acc: 0.4880\n",
            "\n",
            "Epoch 73/99\n",
            "----------\n",
            "train Loss: 0.3399 Acc: 0.8658\n",
            "val Loss: 1.3599 Acc: 0.4982\n",
            "\n",
            "Epoch 74/99\n",
            "----------\n",
            "train Loss: 0.3275 Acc: 0.8728\n",
            "val Loss: 1.4013 Acc: 0.4968\n",
            "\n",
            "Epoch 75/99\n",
            "----------\n",
            "train Loss: 0.3315 Acc: 0.8722\n",
            "val Loss: 1.3784 Acc: 0.4991\n",
            "\n",
            "Epoch 76/99\n",
            "----------\n",
            "train Loss: 0.3221 Acc: 0.8738\n",
            "val Loss: 1.3928 Acc: 0.4919\n",
            "\n",
            "Epoch 77/99\n",
            "----------\n",
            "train Loss: 0.3161 Acc: 0.8766\n",
            "val Loss: 1.3856 Acc: 0.4994\n",
            "\n",
            "Epoch 78/99\n",
            "----------\n",
            "train Loss: 0.3183 Acc: 0.8750\n",
            "val Loss: 1.3865 Acc: 0.5077\n",
            "\n",
            "Epoch 79/99\n",
            "----------\n",
            "train Loss: 0.3128 Acc: 0.8784\n",
            "val Loss: 1.4229 Acc: 0.4936\n",
            "\n",
            "Epoch 80/99\n",
            "----------\n",
            "train Loss: 0.3151 Acc: 0.8784\n",
            "val Loss: 1.4033 Acc: 0.5006\n",
            "\n",
            "Epoch 81/99\n",
            "----------\n",
            "train Loss: 0.3108 Acc: 0.8825\n",
            "val Loss: 1.4228 Acc: 0.4905\n",
            "\n",
            "Epoch 82/99\n",
            "----------\n",
            "train Loss: 0.2976 Acc: 0.8834\n",
            "val Loss: 1.4384 Acc: 0.4868\n",
            "\n",
            "Epoch 83/99\n",
            "----------\n",
            "train Loss: 0.2922 Acc: 0.8872\n",
            "val Loss: 1.4525 Acc: 0.4898\n",
            "\n",
            "Epoch 84/99\n",
            "----------\n",
            "train Loss: 0.2884 Acc: 0.8880\n",
            "val Loss: 1.4252 Acc: 0.4955\n",
            "\n",
            "Epoch 85/99\n",
            "----------\n",
            "train Loss: 0.2863 Acc: 0.8920\n",
            "val Loss: 1.4630 Acc: 0.4829\n",
            "\n",
            "Epoch 86/99\n",
            "----------\n",
            "train Loss: 0.2939 Acc: 0.8889\n",
            "val Loss: 1.4457 Acc: 0.5018\n",
            "\n",
            "Epoch 87/99\n",
            "----------\n",
            "train Loss: 0.2921 Acc: 0.8881\n",
            "val Loss: 1.4192 Acc: 0.4899\n",
            "\n",
            "Epoch 88/99\n",
            "----------\n",
            "train Loss: 0.2743 Acc: 0.8959\n",
            "val Loss: 1.4804 Acc: 0.4934\n",
            "\n",
            "Epoch 89/99\n",
            "----------\n",
            "train Loss: 0.2841 Acc: 0.8899\n",
            "val Loss: 1.4430 Acc: 0.4910\n",
            "\n",
            "Epoch 90/99\n",
            "----------\n",
            "train Loss: 0.2774 Acc: 0.8951\n",
            "val Loss: 1.4583 Acc: 0.4870\n",
            "\n",
            "Epoch 91/99\n",
            "----------\n",
            "train Loss: 0.2716 Acc: 0.8934\n",
            "val Loss: 1.4503 Acc: 0.4987\n",
            "\n",
            "Epoch 92/99\n",
            "----------\n",
            "train Loss: 0.2671 Acc: 0.8991\n",
            "val Loss: 1.4661 Acc: 0.5027\n",
            "\n",
            "Epoch 93/99\n",
            "----------\n",
            "train Loss: 0.2597 Acc: 0.9016\n",
            "val Loss: 1.4896 Acc: 0.4826\n",
            "\n",
            "Epoch 94/99\n",
            "----------\n",
            "train Loss: 0.2573 Acc: 0.9028\n",
            "val Loss: 1.4870 Acc: 0.4871\n",
            "\n",
            "Epoch 95/99\n",
            "----------\n",
            "train Loss: 0.2594 Acc: 0.9030\n",
            "val Loss: 1.4839 Acc: 0.4990\n",
            "\n",
            "Epoch 96/99\n",
            "----------\n",
            "train Loss: 0.2550 Acc: 0.9031\n",
            "val Loss: 1.4796 Acc: 0.4950\n",
            "\n",
            "Epoch 97/99\n",
            "----------\n",
            "train Loss: 0.2507 Acc: 0.9028\n",
            "val Loss: 1.5215 Acc: 0.4886\n",
            "\n",
            "Epoch 98/99\n",
            "----------\n",
            "train Loss: 0.2444 Acc: 0.9040\n",
            "val Loss: 1.4770 Acc: 0.5017\n",
            "\n",
            "Epoch 99/99\n",
            "----------\n",
            "train Loss: 0.2466 Acc: 0.9024\n",
            "val Loss: 1.5032 Acc: 0.4898\n",
            "\n",
            "Training complete in 2m 46s\n",
            "Best val Loss: 1.122649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "SFnswSmvgnwA",
        "outputId": "3be6ee0d-f0aa-4ad1-b4f0-861b93a15420"
      },
      "source": [
        "plt.plot(all_train_loss)\n",
        "plt.plot(all_val_loss)\n",
        "plt.legend(['train', 'val'])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f33f01d2810>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yluuG3bBmE40",
        "outputId": "7544bfe3-a05e-4233-c9f9-106db030abbc"
      },
      "source": [
        "datatestiter = iter(testloader)\n",
        "input_test, labels_test = datatestiter.next()\n",
        "input_test = input_test.to(device)\n",
        "labels_test = labels_test.to(device)\n",
        "model.eval()\n",
        "output_test = model(input_test)\n",
        "_, preds = torch.max(output_test, 1)\n",
        "print(preds.shape)\n",
        "print(labels_test.shape)\n",
        "print('preds',preds[0:8])\n",
        "print('labels', labels_test[0:8])\n",
        "print(torch.sum(preds == labels_test) / len(labels_test))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "preds tensor([1, 3, 2, 0, 3, 1, 3, 3], device='cuda:0')\n",
            "labels tensor([0., 1., 2., 2., 0., 1., 3., 3.], device='cuda:0')\n",
            "tensor(0.5156, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}