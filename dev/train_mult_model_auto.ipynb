{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_mult_model_auto.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl-v8TM9SqJM",
        "outputId": "97287b0b-4299-4ef7-f888-0547660aa45c"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "from torch import Tensor\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.fft import rfft, rfftfreq, fft, fftfreq\n",
        "import scipy\n",
        "import time\n",
        "import copy\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load in Data\n",
        "with open('/content/drive/MyDrive/CSE 481 Capstone/processed_data.npy', 'rb') as f:\n",
        "    data = np.load(f)\n",
        "print(data.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(17920, 4, 32, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0dpGYfSSwiu",
        "outputId": "f107ed9a-24bd-410a-9124-8afe75e1216d"
      },
      "source": [
        "# Load in Labels\n",
        "labels = pd.read_excel(\"/content/drive/MyDrive/CSE 481 Capstone/metadata_xls/participant_ratings.xls\")\n",
        "print(labels.head())\n",
        "sub_labels = []\n",
        "for i in range(len(labels)):\n",
        "  sub_labels.append([labels.loc[i, 'Valence'], labels.loc[i, 'Arousal']])\n",
        "sub_labels = np.array(sub_labels)\n",
        "print(sub_labels.shape)\n",
        "print(sub_labels)\n",
        "sub_labels_2 = np.zeros((len(sub_labels)))\n",
        "for i in range(len(sub_labels)):\n",
        "  instance = sub_labels[i]\n",
        "  valence = instance[0]\n",
        "  arousal = instance[1]\n",
        "  if (valence < 5 and arousal < 5):\n",
        "    sub_labels_2[i] = 0\n",
        "  elif (valence < 5 and arousal >= 5):\n",
        "    sub_labels_2[i] = 1\n",
        "  elif (valence >= 5 and arousal < 5):\n",
        "    sub_labels_2[i] = 2\n",
        "  else:\n",
        "    sub_labels_2[i] = 3\n",
        "#lb = preprocessing.LabelBinarizer()\n",
        "#sub_labels_2 = lb.fit_transform(sub_labels_2)\n",
        "print(sub_labels_2)\n",
        "\n",
        "# convert to windowed labels\n",
        "data_labels = np.repeat(sub_labels_2, 14, axis=0)\n",
        "print(data_labels.shape)\n",
        "print(data_labels)\n",
        "\n",
        "#convert to tensor\n",
        "#ata = torch.tensor(data)\n",
        "data = torch.from_numpy(data).float()\n",
        "data_labels = torch.from_numpy(data_labels).float()\n",
        "print(data.dtype)\n",
        "print(data.shape)\n",
        "dataset = TensorDataset(Tensor(data) , Tensor(data_labels))\n",
        "\n",
        "print(data[29][1])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Participant_id  Trial  Experiment_id  ...  Dominance  Liking  Familiarity\n",
            "0               1      1              5  ...       7.19    6.05          4.0\n",
            "1               1      2             18  ...       6.94    8.01          4.0\n",
            "2               1      3              4  ...       6.12    8.06          4.0\n",
            "3               1      4             24  ...       8.01    8.22          4.0\n",
            "4               1      5             20  ...       7.19    8.13          1.0\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "(1280, 2)\n",
            "[[6.96 3.92]\n",
            " [7.23 7.15]\n",
            " [4.94 6.01]\n",
            " ...\n",
            " [8.05 7.09]\n",
            " [4.01 7.17]\n",
            " [4.08 5.95]]\n",
            "[2. 3. 1. ... 3. 1. 1.]\n",
            "(17920,)\n",
            "[2. 2. 2. ... 1. 1. 1.]\n",
            "torch.float32\n",
            "torch.Size([17920, 4, 32, 32])\n",
            "tensor([[ 1.0000,  0.6335,  0.4222,  ..., -0.0332,  0.3070,  0.1884],\n",
            "        [ 0.6335,  1.0000,  0.8858,  ..., -0.0963,  0.4528,  0.2397],\n",
            "        [ 0.4222,  0.8858,  1.0000,  ...,  0.0902,  0.4818,  0.2817],\n",
            "        ...,\n",
            "        [-0.0332, -0.0963,  0.0902,  ...,  1.0000,  0.4132,  0.5115],\n",
            "        [ 0.3070,  0.4528,  0.4818,  ...,  0.4132,  1.0000,  0.7692],\n",
            "        [ 0.1884,  0.2397,  0.2817,  ...,  0.5115,  0.7692,  1.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwPav16VTITZ",
        "outputId": "3447a6f1-814c-49a5-b424-0bc47a19db1d"
      },
      "source": [
        "pre_train_size = int(0.9 * len(dataset))\n",
        "test_size = len(dataset) - pre_train_size\n",
        "pre_train_set, test_set = torch.utils.data.random_split(dataset, [pre_train_size, test_size])\n",
        "train_size = int(0.8 * len(pre_train_set))\n",
        "val_size = len(pre_train_set) - train_size\n",
        "print(len(pre_train_set))\n",
        "print(train_size)\n",
        "print(val_size)\n",
        "train_set, val_set = torch.utils.data.random_split(pre_train_set, [train_size, val_size])\n",
        "\n",
        "batch_size = 128\n",
        "print(len(train_set), len(val_set), len(test_set))\n",
        "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "dataloaders = {\n",
        "    'train': trainloader,\n",
        "    'val': valloader,\n",
        "}"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16128\n",
            "12902\n",
            "3226\n",
            "12902 3226 1792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A2TPoSsTMXZ",
        "outputId": "e7d81274-d7c5-446c-af42-879a3b1eeddc"
      },
      "source": [
        "# with auto encoder\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# skipped autoencoder\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(4, 32, [3, 1]),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Conv2d(32, 64, [3, 1]),\n",
        "    nn.ReLU(), # Maybe not sure\n",
        "    nn.Dropout(),\n",
        "    nn.MaxPool2d([3, 3]),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(5760, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 4)\n",
        ")\n",
        "model.to(device)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "print(model)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Sequential(\n",
            "  (0): Conv2d(4, 32, kernel_size=[3, 1], stride=(1, 1))\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Conv2d(32, 64, kernel_size=[3, 1], stride=(1, 1))\n",
            "  (4): ReLU()\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): MaxPool2d(kernel_size=[3, 3], stride=[3, 3], padding=0, dilation=1, ceil_mode=False)\n",
            "  (7): Flatten(start_dim=1, end_dim=-1)\n",
            "  (8): Linear(in_features=5760, out_features=512, bias=True)\n",
            "  (9): ReLU()\n",
            "  (10): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (11): ReLU()\n",
            "  (12): Linear(in_features=256, out_features=4, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlUNDjOTTVMv",
        "outputId": "d1dcd566-a5fb-416b-8122-5a1978eff689"
      },
      "source": [
        "since = time.time()\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = 10000.0\n",
        "best_acc = 0\n",
        "all_train_loss = []\n",
        "all_val_loss = []\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in dataloaders[phase]:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            #print(inputs)\n",
        "            #print(labels)\n",
        "            \n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs = model(inputs)\n",
        "                #print(outputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                #print(preds)\n",
        "                #print(outputs.dtype)\n",
        "                #print(labels.dtype)\n",
        "                loss = loss_func(outputs, labels.long())\n",
        "                #print(loss.item())\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                #print(running_loss)\n",
        "                \n",
        "                running_corrects += torch.sum(preds == labels.data) / inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders[phase])\n",
        "        epoch_acc = running_corrects / len(dataloaders[phase])\n",
        "\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "            phase, epoch_loss, epoch_acc))\n",
        "        if phase == 'train':\n",
        "          all_train_loss.append(epoch_loss)\n",
        "        else:\n",
        "          all_val_loss.append(epoch_loss)\n",
        "        # deep copy the model\n",
        "        # if phase == 'val' and epoch_acc > best_acc:\n",
        "        #     best_acc = epoch_acc\n",
        "        #     best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        if phase == 'val' and epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print()\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))\n",
        "print('Best val Loss: {:4f}'.format(best_loss))\n",
        "\n",
        "# load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "# torch.save(model.state_dict(), '/content/drive/MyDrive/CSE 481 Capstone/model_multi.pth')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 1.3636 Acc: 0.3568\n",
            "val Loss: 1.3807 Acc: 0.3342\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 1.3282 Acc: 0.3757\n",
            "val Loss: 1.3566 Acc: 0.3779\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 1.2859 Acc: 0.3949\n",
            "val Loss: 1.3144 Acc: 0.3958\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 1.2657 Acc: 0.4098\n",
            "val Loss: 1.2966 Acc: 0.4025\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 1.2533 Acc: 0.4152\n",
            "val Loss: 1.2827 Acc: 0.4138\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 1.2439 Acc: 0.4262\n",
            "val Loss: 1.2750 Acc: 0.4204\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 1.2366 Acc: 0.4272\n",
            "val Loss: 1.2655 Acc: 0.4249\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 1.2232 Acc: 0.4314\n",
            "val Loss: 1.2582 Acc: 0.4225\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 1.2155 Acc: 0.4368\n",
            "val Loss: 1.2442 Acc: 0.4324\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 1.2044 Acc: 0.4394\n",
            "val Loss: 1.2402 Acc: 0.4439\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 1.1886 Acc: 0.4543\n",
            "val Loss: 1.2321 Acc: 0.4523\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 1.1748 Acc: 0.4602\n",
            "val Loss: 1.2202 Acc: 0.4528\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 1.1613 Acc: 0.4664\n",
            "val Loss: 1.2143 Acc: 0.4586\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 1.1401 Acc: 0.4777\n",
            "val Loss: 1.2030 Acc: 0.4652\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 1.1241 Acc: 0.4848\n",
            "val Loss: 1.2003 Acc: 0.4583\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 1.1117 Acc: 0.4922\n",
            "val Loss: 1.1897 Acc: 0.4721\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 1.0925 Acc: 0.5034\n",
            "val Loss: 1.1838 Acc: 0.4742\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 1.0757 Acc: 0.5187\n",
            "val Loss: 1.1805 Acc: 0.4697\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 1.0594 Acc: 0.5218\n",
            "val Loss: 1.1720 Acc: 0.4676\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 1.0346 Acc: 0.5374\n",
            "val Loss: 1.1596 Acc: 0.4718\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 1.0251 Acc: 0.5469\n",
            "val Loss: 1.1538 Acc: 0.4802\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 1.0053 Acc: 0.5533\n",
            "val Loss: 1.1584 Acc: 0.4766\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 0.9808 Acc: 0.5676\n",
            "val Loss: 1.1565 Acc: 0.4776\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 0.9553 Acc: 0.5806\n",
            "val Loss: 1.1559 Acc: 0.4701\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 0.9339 Acc: 0.5860\n",
            "val Loss: 1.1495 Acc: 0.4718\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 0.9136 Acc: 0.6070\n",
            "val Loss: 1.1450 Acc: 0.4832\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 0.8997 Acc: 0.6127\n",
            "val Loss: 1.1470 Acc: 0.4818\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 0.8773 Acc: 0.6198\n",
            "val Loss: 1.1474 Acc: 0.4770\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 0.8603 Acc: 0.6279\n",
            "val Loss: 1.1382 Acc: 0.4925\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 0.8353 Acc: 0.6376\n",
            "val Loss: 1.1540 Acc: 0.4818\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 0.8281 Acc: 0.6503\n",
            "val Loss: 1.1602 Acc: 0.4794\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 0.8036 Acc: 0.6567\n",
            "val Loss: 1.1553 Acc: 0.4845\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 0.7918 Acc: 0.6653\n",
            "val Loss: 1.1647 Acc: 0.4788\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 0.7698 Acc: 0.6728\n",
            "val Loss: 1.1747 Acc: 0.4728\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 0.7642 Acc: 0.6750\n",
            "val Loss: 1.1688 Acc: 0.4859\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 0.7367 Acc: 0.6866\n",
            "val Loss: 1.1892 Acc: 0.4839\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 0.7281 Acc: 0.6904\n",
            "val Loss: 1.1523 Acc: 0.4973\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 0.7024 Acc: 0.7033\n",
            "val Loss: 1.1695 Acc: 0.4908\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 0.7009 Acc: 0.7060\n",
            "val Loss: 1.1812 Acc: 0.4890\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 0.6906 Acc: 0.7155\n",
            "val Loss: 1.1808 Acc: 0.4902\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 0.6640 Acc: 0.7254\n",
            "val Loss: 1.1907 Acc: 0.4842\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 0.6649 Acc: 0.7286\n",
            "val Loss: 1.1890 Acc: 0.4868\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 0.6507 Acc: 0.7277\n",
            "val Loss: 1.2183 Acc: 0.4713\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 0.6473 Acc: 0.7318\n",
            "val Loss: 1.2003 Acc: 0.4868\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 0.6429 Acc: 0.7356\n",
            "val Loss: 1.1990 Acc: 0.4871\n",
            "\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 0.6297 Acc: 0.7406\n",
            "val Loss: 1.1968 Acc: 0.4838\n",
            "\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 0.6091 Acc: 0.7484\n",
            "val Loss: 1.2210 Acc: 0.4811\n",
            "\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 0.5902 Acc: 0.7581\n",
            "val Loss: 1.2025 Acc: 0.4833\n",
            "\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 0.6000 Acc: 0.7565\n",
            "val Loss: 1.2275 Acc: 0.4850\n",
            "\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 0.5877 Acc: 0.7578\n",
            "val Loss: 1.2431 Acc: 0.4746\n",
            "\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 0.5821 Acc: 0.7636\n",
            "val Loss: 1.2177 Acc: 0.4829\n",
            "\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 0.5682 Acc: 0.7654\n",
            "val Loss: 1.2163 Acc: 0.4847\n",
            "\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 0.5682 Acc: 0.7684\n",
            "val Loss: 1.2074 Acc: 0.4871\n",
            "\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 0.5524 Acc: 0.7752\n",
            "val Loss: 1.2302 Acc: 0.4737\n",
            "\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 0.5457 Acc: 0.7773\n",
            "val Loss: 1.2265 Acc: 0.4850\n",
            "\n",
            "Epoch 55/99\n",
            "----------\n",
            "train Loss: 0.5466 Acc: 0.7772\n",
            "val Loss: 1.2524 Acc: 0.4814\n",
            "\n",
            "Epoch 56/99\n",
            "----------\n",
            "train Loss: 0.5383 Acc: 0.7793\n",
            "val Loss: 1.2368 Acc: 0.4775\n",
            "\n",
            "Epoch 57/99\n",
            "----------\n",
            "train Loss: 0.5296 Acc: 0.7870\n",
            "val Loss: 1.2405 Acc: 0.4893\n",
            "\n",
            "Epoch 58/99\n",
            "----------\n",
            "train Loss: 0.5300 Acc: 0.7889\n",
            "val Loss: 1.2406 Acc: 0.4788\n",
            "\n",
            "Epoch 59/99\n",
            "----------\n",
            "train Loss: 0.5279 Acc: 0.7888\n",
            "val Loss: 1.2514 Acc: 0.4806\n",
            "\n",
            "Epoch 60/99\n",
            "----------\n",
            "train Loss: 0.5156 Acc: 0.7943\n",
            "val Loss: 1.2724 Acc: 0.4739\n",
            "\n",
            "Epoch 61/99\n",
            "----------\n",
            "train Loss: 0.4945 Acc: 0.8028\n",
            "val Loss: 1.2864 Acc: 0.4827\n",
            "\n",
            "Epoch 62/99\n",
            "----------\n",
            "train Loss: 0.4905 Acc: 0.8018\n",
            "val Loss: 1.2779 Acc: 0.4712\n",
            "\n",
            "Epoch 63/99\n",
            "----------\n",
            "train Loss: 0.4863 Acc: 0.8037\n",
            "val Loss: 1.3027 Acc: 0.4746\n",
            "\n",
            "Epoch 64/99\n",
            "----------\n",
            "train Loss: 0.4812 Acc: 0.8070\n",
            "val Loss: 1.2808 Acc: 0.4770\n",
            "\n",
            "Epoch 65/99\n",
            "----------\n",
            "train Loss: 0.4737 Acc: 0.8101\n",
            "val Loss: 1.2759 Acc: 0.4848\n",
            "\n",
            "Epoch 66/99\n",
            "----------\n",
            "train Loss: 0.4684 Acc: 0.8138\n",
            "val Loss: 1.2882 Acc: 0.4791\n",
            "\n",
            "Epoch 67/99\n",
            "----------\n",
            "train Loss: 0.4878 Acc: 0.8030\n",
            "val Loss: 1.3050 Acc: 0.4715\n",
            "\n",
            "Epoch 68/99\n",
            "----------\n",
            "train Loss: 0.4754 Acc: 0.8108\n",
            "val Loss: 1.2940 Acc: 0.4859\n",
            "\n",
            "Epoch 69/99\n",
            "----------\n",
            "train Loss: 0.4569 Acc: 0.8139\n",
            "val Loss: 1.2951 Acc: 0.4878\n",
            "\n",
            "Epoch 70/99\n",
            "----------\n",
            "train Loss: 0.4619 Acc: 0.8156\n",
            "val Loss: 1.3031 Acc: 0.4782\n",
            "\n",
            "Epoch 71/99\n",
            "----------\n",
            "train Loss: 0.4442 Acc: 0.8250\n",
            "val Loss: 1.3177 Acc: 0.4871\n",
            "\n",
            "Epoch 72/99\n",
            "----------\n",
            "train Loss: 0.4382 Acc: 0.8252\n",
            "val Loss: 1.3033 Acc: 0.4755\n",
            "\n",
            "Epoch 73/99\n",
            "----------\n",
            "train Loss: 0.4309 Acc: 0.8282\n",
            "val Loss: 1.2937 Acc: 0.4836\n",
            "\n",
            "Epoch 74/99\n",
            "----------\n",
            "train Loss: 0.4358 Acc: 0.8273\n",
            "val Loss: 1.3415 Acc: 0.4815\n",
            "\n",
            "Epoch 75/99\n",
            "----------\n",
            "train Loss: 0.4242 Acc: 0.8329\n",
            "val Loss: 1.3294 Acc: 0.4787\n",
            "\n",
            "Epoch 76/99\n",
            "----------\n",
            "train Loss: 0.4154 Acc: 0.8334\n",
            "val Loss: 1.3445 Acc: 0.4761\n",
            "\n",
            "Epoch 77/99\n",
            "----------\n",
            "train Loss: 0.4042 Acc: 0.8419\n",
            "val Loss: 1.3511 Acc: 0.4862\n",
            "\n",
            "Epoch 78/99\n",
            "----------\n",
            "train Loss: 0.4063 Acc: 0.8384\n",
            "val Loss: 1.3356 Acc: 0.4784\n",
            "\n",
            "Epoch 79/99\n",
            "----------\n",
            "train Loss: 0.4026 Acc: 0.8385\n",
            "val Loss: 1.3586 Acc: 0.4826\n",
            "\n",
            "Epoch 80/99\n",
            "----------\n",
            "train Loss: 0.4071 Acc: 0.8382\n",
            "val Loss: 1.3691 Acc: 0.4832\n",
            "\n",
            "Epoch 81/99\n",
            "----------\n",
            "train Loss: 0.3861 Acc: 0.8513\n",
            "val Loss: 1.3467 Acc: 0.4875\n",
            "\n",
            "Epoch 82/99\n",
            "----------\n",
            "train Loss: 0.3904 Acc: 0.8443\n",
            "val Loss: 1.3529 Acc: 0.4823\n",
            "\n",
            "Epoch 83/99\n",
            "----------\n",
            "train Loss: 0.3856 Acc: 0.8494\n",
            "val Loss: 1.3418 Acc: 0.4917\n",
            "\n",
            "Epoch 84/99\n",
            "----------\n",
            "train Loss: 0.3811 Acc: 0.8495\n",
            "val Loss: 1.3424 Acc: 0.4896\n",
            "\n",
            "Epoch 85/99\n",
            "----------\n",
            "train Loss: 0.3668 Acc: 0.8558\n",
            "val Loss: 1.3713 Acc: 0.4848\n",
            "\n",
            "Epoch 86/99\n",
            "----------\n",
            "train Loss: 0.3609 Acc: 0.8538\n",
            "val Loss: 1.3646 Acc: 0.4823\n",
            "\n",
            "Epoch 87/99\n",
            "----------\n",
            "train Loss: 0.3647 Acc: 0.8564\n",
            "val Loss: 1.3532 Acc: 0.4974\n",
            "\n",
            "Epoch 88/99\n",
            "----------\n",
            "train Loss: 0.3686 Acc: 0.8594\n",
            "val Loss: 1.3738 Acc: 0.4851\n",
            "\n",
            "Epoch 89/99\n",
            "----------\n",
            "train Loss: 0.3600 Acc: 0.8566\n",
            "val Loss: 1.3735 Acc: 0.4943\n",
            "\n",
            "Epoch 90/99\n",
            "----------\n",
            "train Loss: 0.3584 Acc: 0.8612\n",
            "val Loss: 1.3847 Acc: 0.4907\n",
            "\n",
            "Epoch 91/99\n",
            "----------\n",
            "train Loss: 0.3579 Acc: 0.8609\n",
            "val Loss: 1.3657 Acc: 0.5021\n",
            "\n",
            "Epoch 92/99\n",
            "----------\n",
            "train Loss: 0.3512 Acc: 0.8644\n",
            "val Loss: 1.3699 Acc: 0.4854\n",
            "\n",
            "Epoch 93/99\n",
            "----------\n",
            "train Loss: 0.3508 Acc: 0.8602\n",
            "val Loss: 1.3804 Acc: 0.4880\n",
            "\n",
            "Epoch 94/99\n",
            "----------\n",
            "train Loss: 0.3536 Acc: 0.8645\n",
            "val Loss: 1.3890 Acc: 0.4928\n",
            "\n",
            "Epoch 95/99\n",
            "----------\n",
            "train Loss: 0.3363 Acc: 0.8718\n",
            "val Loss: 1.3763 Acc: 0.4961\n",
            "\n",
            "Epoch 96/99\n",
            "----------\n",
            "train Loss: 0.3365 Acc: 0.8724\n",
            "val Loss: 1.3759 Acc: 0.4932\n",
            "\n",
            "Epoch 97/99\n",
            "----------\n",
            "train Loss: 0.3345 Acc: 0.8680\n",
            "val Loss: 1.3948 Acc: 0.4908\n",
            "\n",
            "Epoch 98/99\n",
            "----------\n",
            "train Loss: 0.3296 Acc: 0.8718\n",
            "val Loss: 1.3971 Acc: 0.4856\n",
            "\n",
            "Epoch 99/99\n",
            "----------\n",
            "train Loss: 0.3322 Acc: 0.8683\n",
            "val Loss: 1.3780 Acc: 0.4938\n",
            "\n",
            "Training complete in 2m 40s\n",
            "Best val Loss: 1.138186\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmihUHiTm3Ve",
        "outputId": "336878fe-99b4-48ce-9745-7befbd285e61"
      },
      "source": [
        "#autoencoder\n",
        "print(model)\n",
        "new_model = nn.Sequential(*list(model.children())[:-5])\n",
        "new_model.to(device)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(4, 32, kernel_size=[3, 1], stride=(1, 1))\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Conv2d(32, 64, kernel_size=[3, 1], stride=(1, 1))\n",
            "  (4): ReLU()\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): MaxPool2d(kernel_size=[3, 3], stride=[3, 3], padding=0, dilation=1, ceil_mode=False)\n",
            "  (7): Flatten(start_dim=1, end_dim=-1)\n",
            "  (8): Linear(in_features=5760, out_features=512, bias=True)\n",
            "  (9): ReLU()\n",
            "  (10): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (11): ReLU()\n",
            "  (12): Linear(in_features=256, out_features=4, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(4, 32, kernel_size=[3, 1], stride=(1, 1))\n",
              "  (1): ReLU()\n",
              "  (2): Dropout(p=0.5, inplace=False)\n",
              "  (3): Conv2d(32, 64, kernel_size=[3, 1], stride=(1, 1))\n",
              "  (4): ReLU()\n",
              "  (5): Dropout(p=0.5, inplace=False)\n",
              "  (6): MaxPool2d(kernel_size=[3, 3], stride=[3, 3], padding=0, dilation=1, ceil_mode=False)\n",
              "  (7): Flatten(start_dim=1, end_dim=-1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNLQissntZyB"
      },
      "source": [
        "#autoencoder\n",
        "auto_model = nn.Sequential(\n",
        "    nn.Linear(5760, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 5760)\n",
        ")\n",
        "auto_model.to(device)\n",
        "optimizer2 = torch.optim.Adam(auto_model.parameters(), lr=0.001)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxXwQ8ZPtftS",
        "outputId": "83492df4-2d7e-4903-a61c-83a22473413d"
      },
      "source": [
        "since = time.time()\n",
        "#best_model_wts = copy.deepcopy(auto_model.state_dict())\n",
        "best_acc = 0.0\n",
        "all_train_loss = []\n",
        "all_val_loss = []\n",
        "num_epochs = 50\n",
        "auto_loss_function = nn.MSELoss()\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            auto_model.train()  # Set model to training mode\n",
        "        else:\n",
        "            auto_model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in dataloaders[phase]:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            #print(inputs)\n",
        "            #print(labels)\n",
        "            \n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer2.zero_grad()\n",
        "            \n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                pre_output = new_model(inputs)\n",
        "                outputs = auto_model(pre_output)\n",
        "                # print(inputs.shape)\n",
        "                # print(pre_output.shape)\n",
        "                # print(outputs.shape)\n",
        "                \n",
        "                loss = auto_loss_function(outputs, pre_output)\n",
        "                #print(loss.item())\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer2.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                #print(running_loss)\n",
        "                \n",
        "                #running_corrects += torch.sum(preds == labels.data) / inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders[phase])\n",
        "        #epoch_acc = running_corrects / len(dataloaders[phase])\n",
        "\n",
        "        print('{} Loss: {:.4f}'.format(\n",
        "            phase, epoch_loss))\n",
        "        if phase == 'train':\n",
        "          all_train_loss.append(epoch_loss)\n",
        "        else:\n",
        "          all_val_loss.append(epoch_loss)\n",
        "        # deep copy the model\n",
        "        # if phase == 'val' and epoch_acc > best_acc:\n",
        "        #     best_acc = epoch_acc\n",
        "        #     best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print()\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))\n",
        "#print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "# load best model weights\n",
        "#model.load_state_dict(best_model_wts)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.0044\n",
            "val Loss: 0.0034\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.0031\n",
            "val Loss: 0.0030\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.0029\n",
            "val Loss: 0.0028\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.0027\n",
            "val Loss: 0.0026\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.0026\n",
            "val Loss: 0.0025\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.0025\n",
            "val Loss: 0.0025\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.0025\n",
            "val Loss: 0.0025\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 38/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 39/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 40/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 41/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 42/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 43/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 44/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 45/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 46/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 47/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 48/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Epoch 49/49\n",
            "----------\n",
            "train Loss: 0.0024\n",
            "val Loss: 0.0024\n",
            "\n",
            "Training complete in 1m 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2ee32o9Fj7Q",
        "outputId": "0e63d192-2936-4389-9eb0-f0bd1a70c40c"
      },
      "source": [
        "#autoencoder\n",
        "# cnn_model = nn.Sequential(\n",
        "#     nn.Linear(5760, 512),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(512, 256),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(256, 128),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(128, 4)\n",
        "# )\n",
        "dnn_model = nn.Sequential(*list(model.children())[-5:])\n",
        "optimizer3 = torch.optim.Adam(dnn_model.parameters(), lr=0.001)\n",
        "dnn_model.to(device)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=5760, out_features=512, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=256, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i544wWgaGekC",
        "outputId": "cf860dc4-1bef-4263-c341-2f7e9b25d002"
      },
      "source": [
        "since = time.time()\n",
        "best_model_wts = copy.deepcopy(dnn_model.state_dict())\n",
        "best_loss = 1000\n",
        "all_train_loss = []\n",
        "all_val_loss = []\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            dnn_model.train()  # Set model to training mode\n",
        "        else:\n",
        "            dnn_model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in dataloaders[phase]:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            #print(inputs)\n",
        "            #print(labels)\n",
        "            \n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer3.zero_grad()\n",
        "            \n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                pre_outputs = new_model(inputs)\n",
        "                auto_outputs = auto_model(pre_outputs)\n",
        "                outputs = dnn_model(pre_outputs)\n",
        "                #print(outputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                #print(preds)\n",
        "                #print(outputs.dtype)\n",
        "                #print(labels.dtype)\n",
        "                loss = loss_func(outputs, labels.long())\n",
        "                #print(loss.item())\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer3.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                #print(running_loss)\n",
        "                \n",
        "                running_corrects += torch.sum(preds == labels.data) / inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders[phase])\n",
        "        epoch_acc = running_corrects / len(dataloaders[phase])\n",
        "\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "            phase, epoch_loss, epoch_acc))\n",
        "        if phase == 'train':\n",
        "          all_train_loss.append(epoch_loss)\n",
        "        else:\n",
        "          all_val_loss.append(epoch_loss)\n",
        "        # deep copy the model\n",
        "        # if phase == 'val' and epoch_acc > best_acc:\n",
        "        #     best_acc = epoch_acc\n",
        "        #     best_model_wts = copy.deepcopy(dnn_model.state_dict())\n",
        "        if phase == 'val' and epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_model_wts = copy.deepcopy(dnn_model.state_dict())\n",
        "\n",
        "    print()\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))\n",
        "print('Best val loss: {:4f}'.format(best_loss))\n",
        "\n",
        "# load best model weights\n",
        "dnn_model.load_state_dict(best_model_wts)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.7091 Acc: 0.7130\n",
            "val Loss: 1.2650 Acc: 0.4881\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.5727 Acc: 0.7740\n",
            "val Loss: 1.3600 Acc: 0.4896\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.4656 Acc: 0.8225\n",
            "val Loss: 1.4739 Acc: 0.4929\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.3665 Acc: 0.8667\n",
            "val Loss: 1.6170 Acc: 0.4931\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.2857 Acc: 0.9051\n",
            "val Loss: 1.7966 Acc: 0.4905\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.2308 Acc: 0.9235\n",
            "val Loss: 2.0244 Acc: 0.4856\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.2139 Acc: 0.9267\n",
            "val Loss: 2.1849 Acc: 0.4905\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.2430 Acc: 0.9082\n",
            "val Loss: 2.2634 Acc: 0.4680\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.2666 Acc: 0.8956\n",
            "val Loss: 2.1087 Acc: 0.4869\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.2248 Acc: 0.9129\n",
            "val Loss: 2.4559 Acc: 0.4497\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.1622 Acc: 0.9402\n",
            "val Loss: 2.5653 Acc: 0.4557\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.1376 Acc: 0.9516\n",
            "val Loss: 2.4569 Acc: 0.4833\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.1372 Acc: 0.9517\n",
            "val Loss: 2.5103 Acc: 0.5000\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.1243 Acc: 0.9575\n",
            "val Loss: 2.9944 Acc: 0.4757\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.1280 Acc: 0.9554\n",
            "val Loss: 2.8504 Acc: 0.4974\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.1112 Acc: 0.9615\n",
            "val Loss: 2.9474 Acc: 0.4994\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.1034 Acc: 0.9660\n",
            "val Loss: 2.8696 Acc: 0.5139\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.0971 Acc: 0.9658\n",
            "val Loss: 2.8029 Acc: 0.4938\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.0880 Acc: 0.9698\n",
            "val Loss: 2.8727 Acc: 0.4953\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.0688 Acc: 0.9778\n",
            "val Loss: 3.0673 Acc: 0.4847\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.0590 Acc: 0.9802\n",
            "val Loss: 3.1264 Acc: 0.5025\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.0511 Acc: 0.9847\n",
            "val Loss: 3.2699 Acc: 0.4902\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.0462 Acc: 0.9851\n",
            "val Loss: 3.3265 Acc: 0.4842\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.0362 Acc: 0.9897\n",
            "val Loss: 3.5183 Acc: 0.4902\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.0384 Acc: 0.9878\n",
            "val Loss: 3.6969 Acc: 0.4812\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.0416 Acc: 0.9874\n",
            "val Loss: 3.4717 Acc: 0.4944\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.0338 Acc: 0.9907\n",
            "val Loss: 3.4783 Acc: 0.5169\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.0294 Acc: 0.9908\n",
            "val Loss: 3.5625 Acc: 0.5088\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.0264 Acc: 0.9923\n",
            "val Loss: 3.6535 Acc: 0.5118\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.0241 Acc: 0.9933\n",
            "val Loss: 3.8157 Acc: 0.5006\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.0214 Acc: 0.9949\n",
            "val Loss: 3.7744 Acc: 0.5181\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.0241 Acc: 0.9921\n",
            "val Loss: 3.8003 Acc: 0.4959\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "train Loss: 0.0365 Acc: 0.9891\n",
            "val Loss: 3.7625 Acc: 0.4959\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "train Loss: 0.0359 Acc: 0.9891\n",
            "val Loss: 3.7760 Acc: 0.5076\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "train Loss: 0.0650 Acc: 0.9793\n",
            "val Loss: 3.9081 Acc: 0.5001\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "train Loss: 0.0528 Acc: 0.9817\n",
            "val Loss: 3.7126 Acc: 0.4925\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "train Loss: 0.0584 Acc: 0.9813\n",
            "val Loss: 3.6321 Acc: 0.4974\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "train Loss: 0.0364 Acc: 0.9887\n",
            "val Loss: 3.6599 Acc: 0.5103\n",
            "\n",
            "Epoch 38/49\n",
            "----------\n",
            "train Loss: 0.0222 Acc: 0.9940\n",
            "val Loss: 3.7678 Acc: 0.5015\n",
            "\n",
            "Epoch 39/49\n",
            "----------\n",
            "train Loss: 0.0202 Acc: 0.9942\n",
            "val Loss: 3.8134 Acc: 0.5064\n",
            "\n",
            "Epoch 40/49\n",
            "----------\n",
            "train Loss: 0.0171 Acc: 0.9953\n",
            "val Loss: 3.9447 Acc: 0.5063\n",
            "\n",
            "Epoch 41/49\n",
            "----------\n",
            "train Loss: 0.0203 Acc: 0.9950\n",
            "val Loss: 3.9537 Acc: 0.5172\n",
            "\n",
            "Epoch 42/49\n",
            "----------\n",
            "train Loss: 0.0169 Acc: 0.9957\n",
            "val Loss: 3.9226 Acc: 0.5073\n",
            "\n",
            "Epoch 43/49\n",
            "----------\n",
            "train Loss: 0.0125 Acc: 0.9968\n",
            "val Loss: 3.9622 Acc: 0.5103\n",
            "\n",
            "Epoch 44/49\n",
            "----------\n",
            "train Loss: 0.0182 Acc: 0.9944\n",
            "val Loss: 3.9291 Acc: 0.5118\n",
            "\n",
            "Epoch 45/49\n",
            "----------\n",
            "train Loss: 0.0184 Acc: 0.9943\n",
            "val Loss: 4.0452 Acc: 0.4980\n",
            "\n",
            "Epoch 46/49\n",
            "----------\n",
            "train Loss: 0.0308 Acc: 0.9906\n",
            "val Loss: 4.4742 Acc: 0.4926\n",
            "\n",
            "Epoch 47/49\n",
            "----------\n",
            "train Loss: 0.0484 Acc: 0.9839\n",
            "val Loss: 4.1256 Acc: 0.4899\n",
            "\n",
            "Epoch 48/49\n",
            "----------\n",
            "train Loss: 0.0340 Acc: 0.9890\n",
            "val Loss: 3.9118 Acc: 0.5100\n",
            "\n",
            "Epoch 49/49\n",
            "----------\n",
            "train Loss: 0.0186 Acc: 0.9948\n",
            "val Loss: 4.0646 Acc: 0.5151\n",
            "\n",
            "Training complete in 1m 16s\n",
            "Best val loss: 1.264967\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m9FPFb6tMAr",
        "outputId": "f2dbc128-fd75-4c5b-9d3d-5234ec907c78"
      },
      "source": [
        "datatestiter = iter(testloader)\n",
        "input_test, labels_test = datatestiter.next()\n",
        "input_test = input_test.to(device)\n",
        "labels_test = labels_test.to(device)\n",
        "new_model.eval()\n",
        "auto_model.eval()\n",
        "dnn_model.eval()\n",
        "one = new_model(input_test)\n",
        "two = auto_model(one)\n",
        "output_test = dnn_model(two)\n",
        "_, preds_test = torch.max(output_test, 1)\n",
        "print(preds_test.shape)\n",
        "print(labels_test.shape)\n",
        "print('predictions', preds_test)\n",
        "print('labels', labels_test)\n",
        "print(torch.sum(preds_test == labels_test) / len(preds_test))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128])\n",
            "torch.Size([128])\n",
            "predictions tensor([0, 2, 0, 1, 3, 2, 1, 0, 1, 3, 2, 2, 2, 2, 3, 0, 3, 3, 0, 1, 0, 1, 0, 2,\n",
            "        1, 2, 0, 1, 0, 1, 0, 1, 1, 3, 2, 3, 2, 3, 2, 1, 1, 0, 1, 2, 3, 2, 2, 1,\n",
            "        1, 3, 1, 3, 1, 0, 2, 1, 0, 2, 2, 1, 1, 3, 3, 0, 1, 3, 2, 2, 0, 3, 3, 2,\n",
            "        2, 2, 2, 0, 2, 1, 2, 2, 3, 1, 3, 2, 3, 0, 3, 3, 1, 2, 0, 1, 0, 2, 2, 0,\n",
            "        1, 1, 3, 3, 1, 2, 3, 3, 2, 1, 3, 2, 1, 1, 3, 1, 2, 2, 2, 1, 2, 3, 2, 2,\n",
            "        2, 0, 2, 0, 3, 1, 3, 1], device='cuda:0')\n",
            "labels tensor([3., 2., 2., 3., 0., 2., 0., 1., 3., 3., 1., 3., 3., 0., 0., 2., 3., 2.,\n",
            "        3., 3., 0., 3., 3., 1., 1., 2., 3., 2., 1., 3., 2., 3., 3., 0., 3., 2.,\n",
            "        1., 2., 2., 1., 3., 3., 2., 3., 2., 3., 3., 0., 2., 0., 2., 2., 0., 0.,\n",
            "        0., 1., 3., 1., 3., 3., 0., 3., 1., 3., 1., 3., 2., 0., 2., 2., 0., 1.,\n",
            "        3., 2., 2., 1., 3., 1., 1., 3., 0., 0., 3., 1., 3., 2., 1., 3., 0., 2.,\n",
            "        3., 0., 1., 3., 3., 3., 1., 3., 3., 1., 1., 0., 3., 3., 3., 1., 0., 3.,\n",
            "        0., 3., 0., 0., 0., 1., 3., 3., 0., 2., 0., 0., 3., 0., 1., 3., 1., 3.,\n",
            "        0., 1.], device='cuda:0')\n",
            "tensor(0.2344, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}