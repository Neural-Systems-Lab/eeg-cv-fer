{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_mult_model_auto.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl-v8TM9SqJM",
        "outputId": "fce1a04a-85b3-4a36-8252-c67646662420"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "from torch import Tensor\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.fft import rfft, rfftfreq, fft, fftfreq\n",
        "import scipy\n",
        "import time\n",
        "import copy\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load in Data\n",
        "with open('/content/drive/MyDrive/CSE 481 Capstone/processed_data.npy', 'rb') as f:\n",
        "    data = np.load(f)\n",
        "print(data.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(17920, 4, 32, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0dpGYfSSwiu",
        "outputId": "006a4531-0679-4f9d-a7a7-d5e3adb1f51a"
      },
      "source": [
        "# Load in Labels\n",
        "labels = pd.read_excel(\"/content/drive/MyDrive/CSE 481 Capstone/metadata_xls/participant_ratings.xls\")\n",
        "print(labels.head())\n",
        "sub_labels = []\n",
        "for i in range(len(labels)):\n",
        "  sub_labels.append([labels.loc[i, 'Valence'], labels.loc[i, 'Arousal']])\n",
        "sub_labels = np.array(sub_labels)\n",
        "print(sub_labels.shape)\n",
        "print(sub_labels)\n",
        "sub_labels_2 = np.zeros((len(sub_labels)))\n",
        "for i in range(len(sub_labels)):\n",
        "  instance = sub_labels[i]\n",
        "  valence = instance[0]\n",
        "  arousal = instance[1]\n",
        "  if (valence < 5 and arousal < 5):\n",
        "    sub_labels_2[i] = 0\n",
        "  elif (valence < 5 and arousal >= 5):\n",
        "    sub_labels_2[i] = 1\n",
        "  elif (valence >= 5 and arousal < 5):\n",
        "    sub_labels_2[i] = 2\n",
        "  else:\n",
        "    sub_labels_2[i] = 3\n",
        "#lb = preprocessing.LabelBinarizer()\n",
        "#sub_labels_2 = lb.fit_transform(sub_labels_2)\n",
        "print(sub_labels_2)\n",
        "\n",
        "# convert to windowed labels\n",
        "data_labels = np.repeat(sub_labels_2, 14, axis=0)\n",
        "print(data_labels.shape)\n",
        "print(data_labels)\n",
        "\n",
        "#convert to tensor\n",
        "#ata = torch.tensor(data)\n",
        "data = torch.from_numpy(data).float()\n",
        "data_labels = torch.from_numpy(data_labels).float()\n",
        "print(data.dtype)\n",
        "print(data.shape)\n",
        "dataset = TensorDataset(Tensor(data) , Tensor(data_labels))\n",
        "\n",
        "print(data[29][1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Participant_id  Trial  Experiment_id  ...  Dominance  Liking  Familiarity\n",
            "0               1      1              5  ...       7.19    6.05          4.0\n",
            "1               1      2             18  ...       6.94    8.01          4.0\n",
            "2               1      3              4  ...       6.12    8.06          4.0\n",
            "3               1      4             24  ...       8.01    8.22          4.0\n",
            "4               1      5             20  ...       7.19    8.13          1.0\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "(1280, 2)\n",
            "[[6.96 3.92]\n",
            " [7.23 7.15]\n",
            " [4.94 6.01]\n",
            " ...\n",
            " [8.05 7.09]\n",
            " [4.01 7.17]\n",
            " [4.08 5.95]]\n",
            "[2. 3. 1. ... 3. 1. 1.]\n",
            "(17920,)\n",
            "[2. 2. 2. ... 1. 1. 1.]\n",
            "torch.float32\n",
            "torch.Size([17920, 4, 32, 32])\n",
            "tensor([[ 1.0000,  0.6335,  0.4222,  ..., -0.0332,  0.3070,  0.1884],\n",
            "        [ 0.6335,  1.0000,  0.8858,  ..., -0.0963,  0.4528,  0.2397],\n",
            "        [ 0.4222,  0.8858,  1.0000,  ...,  0.0902,  0.4818,  0.2817],\n",
            "        ...,\n",
            "        [-0.0332, -0.0963,  0.0902,  ...,  1.0000,  0.4132,  0.5115],\n",
            "        [ 0.3070,  0.4528,  0.4818,  ...,  0.4132,  1.0000,  0.7692],\n",
            "        [ 0.1884,  0.2397,  0.2817,  ...,  0.5115,  0.7692,  1.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwPav16VTITZ",
        "outputId": "69e5ef38-e89a-4cf1-b9ed-918d0a743a93"
      },
      "source": [
        "pre_train_size = int(0.9 * len(dataset))\n",
        "test_size = len(dataset) - pre_train_size\n",
        "pre_train_set, test_set = torch.utils.data.random_split(dataset, [pre_train_size, test_size])\n",
        "train_size = int(0.8 * len(pre_train_set))\n",
        "val_size = len(pre_train_set) - train_size\n",
        "print(len(pre_train_set))\n",
        "print(train_size)\n",
        "print(val_size)\n",
        "train_set, val_set = torch.utils.data.random_split(pre_train_set, [train_size, val_size])\n",
        "\n",
        "batch_size = 128\n",
        "print(len(train_set), len(val_set), len(test_set))\n",
        "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)\n",
        "dataloaders = {\n",
        "    'train': trainloader,\n",
        "    'val': valloader,\n",
        "}"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16128\n",
            "12902\n",
            "3226\n",
            "12902 3226 1792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A2TPoSsTMXZ",
        "outputId": "94ce2ced-c21a-48a9-ee8d-dd2a93153a9d"
      },
      "source": [
        "# with auto encoder\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# skipped autoencoder\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(4, 32, [3, 1]),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(),\n",
        "    nn.Conv2d(32, 64, [3, 1]),\n",
        "    nn.ReLU(), # Maybe not sure\n",
        "    nn.Dropout(),\n",
        "    nn.MaxPool2d([3, 3]),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(5760, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 4)\n",
        ")\n",
        "model.to(device)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "print(model)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Sequential(\n",
            "  (0): Conv2d(4, 32, kernel_size=[3, 1], stride=(1, 1))\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Conv2d(32, 64, kernel_size=[3, 1], stride=(1, 1))\n",
            "  (4): ReLU()\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): MaxPool2d(kernel_size=[3, 3], stride=[3, 3], padding=0, dilation=1, ceil_mode=False)\n",
            "  (7): Flatten(start_dim=1, end_dim=-1)\n",
            "  (8): Linear(in_features=5760, out_features=512, bias=True)\n",
            "  (9): ReLU()\n",
            "  (10): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (11): ReLU()\n",
            "  (12): Linear(in_features=256, out_features=4, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlUNDjOTTVMv",
        "outputId": "a3b24439-03f5-4e30-e0db-ebc0f756b704"
      },
      "source": [
        "since = time.time()\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = 10000.0\n",
        "best_acc = 0\n",
        "all_train_loss = []\n",
        "all_val_loss = []\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "        else:\n",
        "            model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in dataloaders[phase]:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            #print(inputs)\n",
        "            #print(labels)\n",
        "            \n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs = model(inputs)\n",
        "                #print(outputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                #print(preds)\n",
        "                #print(outputs.dtype)\n",
        "                #print(labels.dtype)\n",
        "                loss = loss_func(outputs, labels.long())\n",
        "                #print(loss.item())\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                #print(running_loss)\n",
        "                \n",
        "                running_corrects += torch.sum(preds == labels.data) / inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders[phase])\n",
        "        epoch_acc = running_corrects / len(dataloaders[phase])\n",
        "\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "            phase, epoch_loss, epoch_acc))\n",
        "        if phase == 'train':\n",
        "          all_train_loss.append(epoch_loss)\n",
        "        else:\n",
        "          all_val_loss.append(epoch_loss)\n",
        "        # deep copy the model\n",
        "        # if phase == 'val' and epoch_acc > best_acc:\n",
        "        #     best_acc = epoch_acc\n",
        "        #     best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        if phase == 'val' and epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print()\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))\n",
        "print('Best val Loss: {:4f}'.format(best_loss))\n",
        "\n",
        "# load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "# torch.save(model.state_dict(), '/content/drive/MyDrive/CSE 481 Capstone/model_multi.pth')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 1.3838 Acc: 0.3469\n",
            "val Loss: 1.3773 Acc: 0.3580\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 1.3512 Acc: 0.3603\n",
            "val Loss: 1.3675 Acc: 0.3730\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 1.3230 Acc: 0.3787\n",
            "val Loss: 1.3384 Acc: 0.3799\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 1.2857 Acc: 0.3941\n",
            "val Loss: 1.3121 Acc: 0.3931\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 1.2676 Acc: 0.4031\n",
            "val Loss: 1.3048 Acc: 0.3862\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 1.2519 Acc: 0.4168\n",
            "val Loss: 1.2940 Acc: 0.3904\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 1.2385 Acc: 0.4225\n",
            "val Loss: 1.2797 Acc: 0.4011\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 1.2289 Acc: 0.4259\n",
            "val Loss: 1.2729 Acc: 0.4018\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 1.2188 Acc: 0.4326\n",
            "val Loss: 1.2663 Acc: 0.4060\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 1.2092 Acc: 0.4417\n",
            "val Loss: 1.2602 Acc: 0.4069\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 1.1975 Acc: 0.4450\n",
            "val Loss: 1.2466 Acc: 0.4162\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 1.1894 Acc: 0.4511\n",
            "val Loss: 1.2448 Acc: 0.4174\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 1.1755 Acc: 0.4582\n",
            "val Loss: 1.2390 Acc: 0.4168\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 1.1643 Acc: 0.4643\n",
            "val Loss: 1.2290 Acc: 0.4219\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 1.1535 Acc: 0.4732\n",
            "val Loss: 1.2213 Acc: 0.4262\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 1.1391 Acc: 0.4788\n",
            "val Loss: 1.2174 Acc: 0.4258\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 1.1245 Acc: 0.4869\n",
            "val Loss: 1.2127 Acc: 0.4343\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 1.1148 Acc: 0.4978\n",
            "val Loss: 1.2092 Acc: 0.4238\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 1.0994 Acc: 0.5008\n",
            "val Loss: 1.2028 Acc: 0.4385\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 1.0838 Acc: 0.5120\n",
            "val Loss: 1.2031 Acc: 0.4418\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 1.0720 Acc: 0.5203\n",
            "val Loss: 1.1955 Acc: 0.4460\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 1.0544 Acc: 0.5254\n",
            "val Loss: 1.1898 Acc: 0.4466\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 1.0390 Acc: 0.5376\n",
            "val Loss: 1.1888 Acc: 0.4499\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 1.0198 Acc: 0.5463\n",
            "val Loss: 1.1893 Acc: 0.4421\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 1.0027 Acc: 0.5541\n",
            "val Loss: 1.1889 Acc: 0.4502\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 0.9919 Acc: 0.5590\n",
            "val Loss: 1.1920 Acc: 0.4514\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 0.9687 Acc: 0.5717\n",
            "val Loss: 1.1793 Acc: 0.4526\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 0.9531 Acc: 0.5803\n",
            "val Loss: 1.1851 Acc: 0.4439\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 0.9321 Acc: 0.5924\n",
            "val Loss: 1.1831 Acc: 0.4490\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 0.9141 Acc: 0.6008\n",
            "val Loss: 1.1718 Acc: 0.4545\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 0.8959 Acc: 0.6138\n",
            "val Loss: 1.1788 Acc: 0.4508\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 0.8865 Acc: 0.6156\n",
            "val Loss: 1.1754 Acc: 0.4631\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 0.8681 Acc: 0.6273\n",
            "val Loss: 1.1781 Acc: 0.4613\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 0.8556 Acc: 0.6339\n",
            "val Loss: 1.1842 Acc: 0.4666\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 0.8322 Acc: 0.6391\n",
            "val Loss: 1.1884 Acc: 0.4598\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 0.8142 Acc: 0.6490\n",
            "val Loss: 1.2006 Acc: 0.4529\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 0.8042 Acc: 0.6586\n",
            "val Loss: 1.2181 Acc: 0.4553\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 0.7926 Acc: 0.6648\n",
            "val Loss: 1.2170 Acc: 0.4502\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 0.7726 Acc: 0.6736\n",
            "val Loss: 1.2152 Acc: 0.4658\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 0.7655 Acc: 0.6731\n",
            "val Loss: 1.2224 Acc: 0.4538\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 0.7452 Acc: 0.6881\n",
            "val Loss: 1.2159 Acc: 0.4616\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 0.7403 Acc: 0.6930\n",
            "val Loss: 1.2227 Acc: 0.4589\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 0.7216 Acc: 0.6948\n",
            "val Loss: 1.2109 Acc: 0.4633\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 0.7152 Acc: 0.7103\n",
            "val Loss: 1.2293 Acc: 0.4550\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 0.7105 Acc: 0.7042\n",
            "val Loss: 1.2413 Acc: 0.4583\n",
            "\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 0.6956 Acc: 0.7087\n",
            "val Loss: 1.2414 Acc: 0.4550\n",
            "\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 0.6827 Acc: 0.7193\n",
            "val Loss: 1.2369 Acc: 0.4595\n",
            "\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 0.6698 Acc: 0.7202\n",
            "val Loss: 1.2323 Acc: 0.4621\n",
            "\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 0.6661 Acc: 0.7270\n",
            "val Loss: 1.2520 Acc: 0.4634\n",
            "\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 0.6528 Acc: 0.7337\n",
            "val Loss: 1.2379 Acc: 0.4718\n",
            "\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 0.6543 Acc: 0.7277\n",
            "val Loss: 1.2492 Acc: 0.4588\n",
            "\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 0.6507 Acc: 0.7367\n",
            "val Loss: 1.2539 Acc: 0.4615\n",
            "\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 0.6282 Acc: 0.7443\n",
            "val Loss: 1.2464 Acc: 0.4664\n",
            "\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 0.6189 Acc: 0.7453\n",
            "val Loss: 1.2627 Acc: 0.4666\n",
            "\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 0.6217 Acc: 0.7455\n",
            "val Loss: 1.2813 Acc: 0.4568\n",
            "\n",
            "Epoch 55/99\n",
            "----------\n",
            "train Loss: 0.6023 Acc: 0.7509\n",
            "val Loss: 1.2635 Acc: 0.4649\n",
            "\n",
            "Epoch 56/99\n",
            "----------\n",
            "train Loss: 0.5944 Acc: 0.7533\n",
            "val Loss: 1.2876 Acc: 0.4571\n",
            "\n",
            "Epoch 57/99\n",
            "----------\n",
            "train Loss: 0.5947 Acc: 0.7602\n",
            "val Loss: 1.2823 Acc: 0.4577\n",
            "\n",
            "Epoch 58/99\n",
            "----------\n",
            "train Loss: 0.5873 Acc: 0.7575\n",
            "val Loss: 1.2810 Acc: 0.4601\n",
            "\n",
            "Epoch 59/99\n",
            "----------\n",
            "train Loss: 0.5743 Acc: 0.7656\n",
            "val Loss: 1.2961 Acc: 0.4699\n",
            "\n",
            "Epoch 60/99\n",
            "----------\n",
            "train Loss: 0.5690 Acc: 0.7706\n",
            "val Loss: 1.3154 Acc: 0.4603\n",
            "\n",
            "Epoch 61/99\n",
            "----------\n",
            "train Loss: 0.5684 Acc: 0.7719\n",
            "val Loss: 1.2974 Acc: 0.4627\n",
            "\n",
            "Epoch 62/99\n",
            "----------\n",
            "train Loss: 0.5511 Acc: 0.7741\n",
            "val Loss: 1.3508 Acc: 0.4552\n",
            "\n",
            "Epoch 63/99\n",
            "----------\n",
            "train Loss: 0.5573 Acc: 0.7747\n",
            "val Loss: 1.3062 Acc: 0.4583\n",
            "\n",
            "Epoch 64/99\n",
            "----------\n",
            "train Loss: 0.5402 Acc: 0.7814\n",
            "val Loss: 1.3052 Acc: 0.4621\n",
            "\n",
            "Epoch 65/99\n",
            "----------\n",
            "train Loss: 0.5306 Acc: 0.7851\n",
            "val Loss: 1.2889 Acc: 0.4639\n",
            "\n",
            "Epoch 66/99\n",
            "----------\n",
            "train Loss: 0.5432 Acc: 0.7762\n",
            "val Loss: 1.2932 Acc: 0.4577\n",
            "\n",
            "Epoch 67/99\n",
            "----------\n",
            "train Loss: 0.5247 Acc: 0.7905\n",
            "val Loss: 1.3021 Acc: 0.4634\n",
            "\n",
            "Epoch 68/99\n",
            "----------\n",
            "train Loss: 0.5038 Acc: 0.7993\n",
            "val Loss: 1.3296 Acc: 0.4640\n",
            "\n",
            "Epoch 69/99\n",
            "----------\n",
            "train Loss: 0.5115 Acc: 0.7968\n",
            "val Loss: 1.3046 Acc: 0.4592\n",
            "\n",
            "Epoch 70/99\n",
            "----------\n",
            "train Loss: 0.4977 Acc: 0.8008\n",
            "val Loss: 1.3260 Acc: 0.4625\n",
            "\n",
            "Epoch 71/99\n",
            "----------\n",
            "train Loss: 0.4947 Acc: 0.8057\n",
            "val Loss: 1.3108 Acc: 0.4687\n",
            "\n",
            "Epoch 72/99\n",
            "----------\n",
            "train Loss: 0.4821 Acc: 0.8081\n",
            "val Loss: 1.3275 Acc: 0.4657\n",
            "\n",
            "Epoch 73/99\n",
            "----------\n",
            "train Loss: 0.4870 Acc: 0.8063\n",
            "val Loss: 1.3267 Acc: 0.4591\n",
            "\n",
            "Epoch 74/99\n",
            "----------\n",
            "train Loss: 0.4805 Acc: 0.8054\n",
            "val Loss: 1.3233 Acc: 0.4684\n",
            "\n",
            "Epoch 75/99\n",
            "----------\n",
            "train Loss: 0.4737 Acc: 0.8118\n",
            "val Loss: 1.3443 Acc: 0.4573\n",
            "\n",
            "Epoch 76/99\n",
            "----------\n",
            "train Loss: 0.4745 Acc: 0.8101\n",
            "val Loss: 1.3584 Acc: 0.4489\n",
            "\n",
            "Epoch 77/99\n",
            "----------\n",
            "train Loss: 0.4672 Acc: 0.8149\n",
            "val Loss: 1.3331 Acc: 0.4675\n",
            "\n",
            "Epoch 78/99\n",
            "----------\n",
            "train Loss: 0.4610 Acc: 0.8155\n",
            "val Loss: 1.3609 Acc: 0.4618\n",
            "\n",
            "Epoch 79/99\n",
            "----------\n",
            "train Loss: 0.4563 Acc: 0.8207\n",
            "val Loss: 1.3534 Acc: 0.4684\n",
            "\n",
            "Epoch 80/99\n",
            "----------\n",
            "train Loss: 0.4419 Acc: 0.8271\n",
            "val Loss: 1.3661 Acc: 0.4607\n",
            "\n",
            "Epoch 81/99\n",
            "----------\n",
            "train Loss: 0.4534 Acc: 0.8171\n",
            "val Loss: 1.3652 Acc: 0.4582\n",
            "\n",
            "Epoch 82/99\n",
            "----------\n",
            "train Loss: 0.4378 Acc: 0.8233\n",
            "val Loss: 1.3961 Acc: 0.4637\n",
            "\n",
            "Epoch 83/99\n",
            "----------\n",
            "train Loss: 0.4348 Acc: 0.8288\n",
            "val Loss: 1.3881 Acc: 0.4562\n",
            "\n",
            "Epoch 84/99\n",
            "----------\n",
            "train Loss: 0.4231 Acc: 0.8319\n",
            "val Loss: 1.3654 Acc: 0.4630\n",
            "\n",
            "Epoch 85/99\n",
            "----------\n",
            "train Loss: 0.4268 Acc: 0.8291\n",
            "val Loss: 1.3709 Acc: 0.4553\n",
            "\n",
            "Epoch 86/99\n",
            "----------\n",
            "train Loss: 0.4240 Acc: 0.8350\n",
            "val Loss: 1.3655 Acc: 0.4615\n",
            "\n",
            "Epoch 87/99\n",
            "----------\n",
            "train Loss: 0.4143 Acc: 0.8387\n",
            "val Loss: 1.3759 Acc: 0.4684\n",
            "\n",
            "Epoch 88/99\n",
            "----------\n",
            "train Loss: 0.4087 Acc: 0.8355\n",
            "val Loss: 1.4027 Acc: 0.4682\n",
            "\n",
            "Epoch 89/99\n",
            "----------\n",
            "train Loss: 0.4105 Acc: 0.8387\n",
            "val Loss: 1.4027 Acc: 0.4592\n",
            "\n",
            "Epoch 90/99\n",
            "----------\n",
            "train Loss: 0.4071 Acc: 0.8355\n",
            "val Loss: 1.4020 Acc: 0.4672\n",
            "\n",
            "Epoch 91/99\n",
            "----------\n",
            "train Loss: 0.3957 Acc: 0.8451\n",
            "val Loss: 1.3981 Acc: 0.4612\n",
            "\n",
            "Epoch 92/99\n",
            "----------\n",
            "train Loss: 0.3958 Acc: 0.8471\n",
            "val Loss: 1.4177 Acc: 0.4724\n",
            "\n",
            "Epoch 93/99\n",
            "----------\n",
            "train Loss: 0.3852 Acc: 0.8495\n",
            "val Loss: 1.4104 Acc: 0.4666\n",
            "\n",
            "Epoch 94/99\n",
            "----------\n",
            "train Loss: 0.3879 Acc: 0.8475\n",
            "val Loss: 1.4389 Acc: 0.4597\n",
            "\n",
            "Epoch 95/99\n",
            "----------\n",
            "train Loss: 0.3713 Acc: 0.8532\n",
            "val Loss: 1.4525 Acc: 0.4643\n",
            "\n",
            "Epoch 96/99\n",
            "----------\n",
            "train Loss: 0.3791 Acc: 0.8511\n",
            "val Loss: 1.4419 Acc: 0.4627\n",
            "\n",
            "Epoch 97/99\n",
            "----------\n",
            "train Loss: 0.3812 Acc: 0.8510\n",
            "val Loss: 1.4415 Acc: 0.4780\n",
            "\n",
            "Epoch 98/99\n",
            "----------\n",
            "train Loss: 0.3836 Acc: 0.8489\n",
            "val Loss: 1.4429 Acc: 0.4642\n",
            "\n",
            "Epoch 99/99\n",
            "----------\n",
            "train Loss: 0.3658 Acc: 0.8553\n",
            "val Loss: 1.4638 Acc: 0.4709\n",
            "\n",
            "Training complete in 2m 41s\n",
            "Best val Loss: 1.171820\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmihUHiTm3Ve",
        "outputId": "51696d25-bd6f-4c85-b9dc-5ac29fa469b2"
      },
      "source": [
        "#autoencoder\n",
        "print(model)\n",
        "new_model = nn.Sequential(*list(model.children())[:-5])\n",
        "new_model.to(device)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(4, 32, kernel_size=[3, 1], stride=(1, 1))\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Conv2d(32, 64, kernel_size=[3, 1], stride=(1, 1))\n",
            "  (4): ReLU()\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): MaxPool2d(kernel_size=[3, 3], stride=[3, 3], padding=0, dilation=1, ceil_mode=False)\n",
            "  (7): Flatten(start_dim=1, end_dim=-1)\n",
            "  (8): Linear(in_features=5760, out_features=512, bias=True)\n",
            "  (9): ReLU()\n",
            "  (10): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (11): ReLU()\n",
            "  (12): Linear(in_features=256, out_features=4, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(4, 32, kernel_size=[3, 1], stride=(1, 1))\n",
              "  (1): ReLU()\n",
              "  (2): Dropout(p=0.5, inplace=False)\n",
              "  (3): Conv2d(32, 64, kernel_size=[3, 1], stride=(1, 1))\n",
              "  (4): ReLU()\n",
              "  (5): Dropout(p=0.5, inplace=False)\n",
              "  (6): MaxPool2d(kernel_size=[3, 3], stride=[3, 3], padding=0, dilation=1, ceil_mode=False)\n",
              "  (7): Flatten(start_dim=1, end_dim=-1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNLQissntZyB"
      },
      "source": [
        "#autoencoder\n",
        "auto_model = nn.Sequential(\n",
        "    nn.Linear(5760, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 5760)\n",
        ")\n",
        "auto_model.to(device)\n",
        "optimizer2 = torch.optim.Adam(auto_model.parameters(), lr=0.001)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxXwQ8ZPtftS",
        "outputId": "128f9ece-ef34-44d4-b306-ec2c831eb3dc"
      },
      "source": [
        "since = time.time()\n",
        "#best_model_wts = copy.deepcopy(auto_model.state_dict())\n",
        "best_acc = 0.0\n",
        "all_train_loss = []\n",
        "all_val_loss = []\n",
        "num_epochs = 50\n",
        "auto_loss_function = nn.MSELoss()\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            auto_model.train()  # Set model to training mode\n",
        "        else:\n",
        "            auto_model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in dataloaders[phase]:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            #print(inputs)\n",
        "            #print(labels)\n",
        "            \n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer2.zero_grad()\n",
        "            \n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                pre_output = new_model(inputs)\n",
        "                outputs = auto_model(pre_output)\n",
        "                # print(inputs.shape)\n",
        "                # print(pre_output.shape)\n",
        "                # print(outputs.shape)\n",
        "                \n",
        "                loss = auto_loss_function(outputs, pre_output)\n",
        "                #print(loss.item())\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer2.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                #print(running_loss)\n",
        "                \n",
        "                #running_corrects += torch.sum(preds == labels.data) / inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders[phase])\n",
        "        #epoch_acc = running_corrects / len(dataloaders[phase])\n",
        "\n",
        "        print('{} Loss: {:.4f}'.format(\n",
        "            phase, epoch_loss))\n",
        "        if phase == 'train':\n",
        "          all_train_loss.append(epoch_loss)\n",
        "        else:\n",
        "          all_val_loss.append(epoch_loss)\n",
        "        # deep copy the model\n",
        "        # if phase == 'val' and epoch_acc > best_acc:\n",
        "        #     best_acc = epoch_acc\n",
        "        #     best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print()\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))\n",
        "#print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "# load best model weights\n",
        "#model.load_state_dict(best_model_wts)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.0100\n",
            "val Loss: 0.0078\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.0071\n",
            "val Loss: 0.0067\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.0066\n",
            "val Loss: 0.0065\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.0064\n",
            "val Loss: 0.0063\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.0063\n",
            "val Loss: 0.0062\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.0062\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.0061\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.0061\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.0061\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.0061\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.0061\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.0061\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.0061\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 38/49\n",
            "----------\n",
            "train Loss: 0.0061\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 39/49\n",
            "----------\n",
            "train Loss: 0.0061\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 40/49\n",
            "----------\n",
            "train Loss: 0.0061\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 41/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 42/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 43/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 44/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 45/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 46/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 47/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 48/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Epoch 49/49\n",
            "----------\n",
            "train Loss: 0.0060\n",
            "val Loss: 0.0061\n",
            "\n",
            "Training complete in 1m 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2ee32o9Fj7Q",
        "outputId": "43ca7c12-a2de-48d8-bcb6-925c61fbbc77"
      },
      "source": [
        "#autoencoder\n",
        "# cnn_model = nn.Sequential(\n",
        "#     nn.Linear(5760, 512),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(512, 256),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(256, 128),\n",
        "#     nn.ReLU(),\n",
        "#     nn.Linear(128, 4)\n",
        "# )\n",
        "dnn_model = nn.Sequential(*list(model.children())[-5:])\n",
        "optimizer3 = torch.optim.Adam(dnn_model.parameters(), lr=0.001)\n",
        "dnn_model.to(device)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=5760, out_features=512, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=256, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i544wWgaGekC",
        "outputId": "eeb1e7e5-aa07-4124-d5c0-fe873411937e"
      },
      "source": [
        "since = time.time()\n",
        "best_model_wts = copy.deepcopy(dnn_model.state_dict())\n",
        "best_loss = 1000\n",
        "all_train_loss = []\n",
        "all_val_loss = []\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            dnn_model.train()  # Set model to training mode\n",
        "        else:\n",
        "            dnn_model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in dataloaders[phase]:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            #print(inputs)\n",
        "            #print(labels)\n",
        "            \n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer3.zero_grad()\n",
        "            \n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                pre_outputs = new_model(inputs)\n",
        "                auto_outputs = auto_model(pre_outputs)\n",
        "                outputs = dnn_model(pre_outputs)\n",
        "                #print(outputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                #print(preds)\n",
        "                #print(outputs.dtype)\n",
        "                #print(labels.dtype)\n",
        "                loss = loss_func(outputs, labels.long())\n",
        "                #print(loss.item())\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer3.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                #print(running_loss)\n",
        "                \n",
        "                running_corrects += torch.sum(preds == labels.data) / inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders[phase])\n",
        "        epoch_acc = running_corrects / len(dataloaders[phase])\n",
        "\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "            phase, epoch_loss, epoch_acc))\n",
        "        if phase == 'train':\n",
        "          all_train_loss.append(epoch_loss)\n",
        "        else:\n",
        "          all_val_loss.append(epoch_loss)\n",
        "        # deep copy the model\n",
        "        # if phase == 'val' and epoch_acc > best_acc:\n",
        "        #     best_acc = epoch_acc\n",
        "        #     best_model_wts = copy.deepcopy(dnn_model.state_dict())\n",
        "        if phase == 'val' and epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print()\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))\n",
        "print('Best val loss: {:4f}'.format(best_loss))\n",
        "\n",
        "# load best model weights\n",
        "dnn_model.load_state_dict(best_model_wts)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.8064 Acc: 0.6677\n",
            "val Loss: 1.2826 Acc: 0.4688\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.7010 Acc: 0.7202\n",
            "val Loss: 1.3372 Acc: 0.4739\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.6172 Acc: 0.7577\n",
            "val Loss: 1.4081 Acc: 0.4685\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.5395 Acc: 0.7943\n",
            "val Loss: 1.4923 Acc: 0.4763\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.4641 Acc: 0.8317\n",
            "val Loss: 1.5891 Acc: 0.4745\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.3968 Acc: 0.8642\n",
            "val Loss: 1.7249 Acc: 0.4754\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.3427 Acc: 0.8869\n",
            "val Loss: 1.8659 Acc: 0.4743\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.3044 Acc: 0.8988\n",
            "val Loss: 2.0129 Acc: 0.4736\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.2917 Acc: 0.8956\n",
            "val Loss: 2.1210 Acc: 0.4607\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.2965 Acc: 0.8880\n",
            "val Loss: 2.2944 Acc: 0.4646\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.3192 Acc: 0.8772\n",
            "val Loss: 2.4652 Acc: 0.4733\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.3178 Acc: 0.8779\n",
            "val Loss: 2.3468 Acc: 0.4709\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.2819 Acc: 0.8947\n",
            "val Loss: 2.2950 Acc: 0.4568\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.2439 Acc: 0.9110\n",
            "val Loss: 2.3238 Acc: 0.4700\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.2246 Acc: 0.9183\n",
            "val Loss: 2.4562 Acc: 0.4598\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.2144 Acc: 0.9230\n",
            "val Loss: 2.5121 Acc: 0.4673\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.2178 Acc: 0.9202\n",
            "val Loss: 2.6593 Acc: 0.4560\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.1931 Acc: 0.9300\n",
            "val Loss: 2.7509 Acc: 0.4382\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.1810 Acc: 0.9341\n",
            "val Loss: 2.6915 Acc: 0.4403\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.1611 Acc: 0.9415\n",
            "val Loss: 2.7089 Acc: 0.4643\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.1423 Acc: 0.9520\n",
            "val Loss: 2.8135 Acc: 0.4715\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.1286 Acc: 0.9556\n",
            "val Loss: 2.9288 Acc: 0.4640\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.1249 Acc: 0.9570\n",
            "val Loss: 3.0369 Acc: 0.4640\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.1211 Acc: 0.9601\n",
            "val Loss: 3.0897 Acc: 0.4631\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.1200 Acc: 0.9582\n",
            "val Loss: 3.0732 Acc: 0.4579\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.1117 Acc: 0.9622\n",
            "val Loss: 3.1531 Acc: 0.4597\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.1017 Acc: 0.9648\n",
            "val Loss: 3.2395 Acc: 0.4475\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.0998 Acc: 0.9665\n",
            "val Loss: 3.5245 Acc: 0.4406\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.0923 Acc: 0.9704\n",
            "val Loss: 3.7038 Acc: 0.4489\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.0946 Acc: 0.9666\n",
            "val Loss: 3.6894 Acc: 0.4601\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.0830 Acc: 0.9737\n",
            "val Loss: 3.9291 Acc: 0.4742\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.0813 Acc: 0.9734\n",
            "val Loss: 3.7591 Acc: 0.4841\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "train Loss: 0.0927 Acc: 0.9689\n",
            "val Loss: 3.6883 Acc: 0.4783\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "train Loss: 0.0789 Acc: 0.9720\n",
            "val Loss: 3.7007 Acc: 0.4814\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "train Loss: 0.0650 Acc: 0.9781\n",
            "val Loss: 3.8983 Acc: 0.4816\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "train Loss: 0.0672 Acc: 0.9769\n",
            "val Loss: 3.9909 Acc: 0.4684\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "train Loss: 0.0603 Acc: 0.9803\n",
            "val Loss: 4.6294 Acc: 0.4714\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "train Loss: 0.0661 Acc: 0.9781\n",
            "val Loss: 4.2281 Acc: 0.4750\n",
            "\n",
            "Epoch 38/49\n",
            "----------\n",
            "train Loss: 0.0700 Acc: 0.9766\n",
            "val Loss: 4.1465 Acc: 0.4835\n",
            "\n",
            "Epoch 39/49\n",
            "----------\n",
            "train Loss: 0.0585 Acc: 0.9808\n",
            "val Loss: 4.3312 Acc: 0.4942\n",
            "\n",
            "Epoch 40/49\n",
            "----------\n",
            "train Loss: 0.0565 Acc: 0.9816\n",
            "val Loss: 4.6449 Acc: 0.4972\n",
            "\n",
            "Epoch 41/49\n",
            "----------\n",
            "train Loss: 0.0529 Acc: 0.9828\n",
            "val Loss: 4.7090 Acc: 0.4981\n",
            "\n",
            "Epoch 42/49\n",
            "----------\n",
            "train Loss: 0.0499 Acc: 0.9841\n",
            "val Loss: 4.3527 Acc: 0.4981\n",
            "\n",
            "Epoch 43/49\n",
            "----------\n",
            "train Loss: 0.0496 Acc: 0.9847\n",
            "val Loss: 4.4410 Acc: 0.4895\n",
            "\n",
            "Epoch 44/49\n",
            "----------\n",
            "train Loss: 0.0475 Acc: 0.9844\n",
            "val Loss: 4.4782 Acc: 0.4880\n",
            "\n",
            "Epoch 45/49\n",
            "----------\n",
            "train Loss: 0.0403 Acc: 0.9872\n",
            "val Loss: 4.5018 Acc: 0.4835\n",
            "\n",
            "Epoch 46/49\n",
            "----------\n",
            "train Loss: 0.0435 Acc: 0.9853\n",
            "val Loss: 4.5218 Acc: 0.4912\n",
            "\n",
            "Epoch 47/49\n",
            "----------\n",
            "train Loss: 0.0397 Acc: 0.9881\n",
            "val Loss: 4.6786 Acc: 0.4969\n",
            "\n",
            "Epoch 48/49\n",
            "----------\n",
            "train Loss: 0.0334 Acc: 0.9902\n",
            "val Loss: 4.3183 Acc: 0.4927\n",
            "\n",
            "Epoch 49/49\n",
            "----------\n",
            "train Loss: 0.0364 Acc: 0.9889\n",
            "val Loss: 4.6025 Acc: 0.4856\n",
            "\n",
            "Training complete in 1m 16s\n",
            "Best val loss: 1.282649\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-7507f33455cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# load best model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mdnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_wts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1224\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Sequential:\n\tMissing key(s) in state_dict: \"2.weight\", \"2.bias\", \"4.weight\", \"4.bias\". \n\tUnexpected key(s) in state_dict: \"8.weight\", \"8.bias\", \"10.weight\", \"10.bias\", \"12.weight\", \"12.bias\", \"3.weight\", \"3.bias\". \n\tsize mismatch for 0.weight: copying a param with shape torch.Size([32, 4, 3, 1]) from checkpoint, the shape in current model is torch.Size([512, 5760]).\n\tsize mismatch for 0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([512])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m9FPFb6tMAr"
      },
      "source": [
        "datatestiter = iter(testloader)\n",
        "input_test, labels_test = datatestiter.next()\n",
        "input_test = input_test.to(device)\n",
        "labels_test = labels_test.to(device)\n",
        "new_model.eval()\n",
        "auto_model.eval()\n",
        "dnn_model.eval()\n",
        "one = new_model(input_test)\n",
        "two = auto_model(one)\n",
        "output_test = dnn_model(two)\n",
        "_, preds_test = torch.max(output_test, 1)\n",
        "print(preds_test.shape)\n",
        "print(labels_test.shape)\n",
        "print('predictions', preds_test)\n",
        "print('labels', labels_test)\n",
        "print(torch.sum(preds_test == labels_test) / len(preds_test))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}